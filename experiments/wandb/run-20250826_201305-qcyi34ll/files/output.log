=== LAMB Sweep Run ===
LR: 0.000447
Weight Decay: 0.1410
Warmup Epochs: 2
Using device: cuda
Available GPUs: 4
‚è±Ô∏è  Creating train dataset...
Loading QuickDraw dataset from: ../data/quickdraw_parquet
Available classes: 344
Using 50 classes: ['aircraft carrier', 'arm', 'asparagus', 'backpack', 'banana', 'basketball', 'bottlecap', 'bread', 'broom', 'bulldozer']...
Using per-class Parquet format (recommended)...
  Loading 50 classes concurrently from per-class files...
    aircraft carrier: 1200 samples
    arm: 1200 samples
    asparagus: 1200 samples
    backpack: 1200 samples
    banana: 1200 samples
    basketball: 1200 samples
    bottlecap: 1200 samples
    bread: 1200 samples
    broom: 1200 samples
    bulldozer: 1200 samples
    butterfly: 1200 samples
    camel: 1200 samples
    canoe: 1200 samples
    chair: 1200 samples
    compass: 1200 samples
    cookie: 1200 samples
    drums: 1200 samples
    eyeglasses: 1200 samples
    face: 1200 samples
    fan: 1200 samples
    fence: 1200 samples
    fish: 1200 samples
    flying saucer: 1200 samples
    grapes: 1200 samples
    hand: 1200 samples
    hat: 1200 samples
    horse: 1200 samples
    light bulb: 1200 samples
    lighthouse: 1200 samples
    line: 1200 samples
    marker: 1200 samples
    mountain: 1200 samples
    mouse: 1200 samples
    parachute: 1200 samples
    passport: 1200 samples
    pliers: 1200 samples
    potato: 1200 samples
    sea turtle: 1200 samples
    snowflake: 1200 samples
    spider: 1200 samples
    square: 1200 samples
    steak: 1200 samples
    swing set: 1200 samples
    sword: 1200 samples
    television: 1200 samples
    tennis racquet: 1200 samples
    toothbrush: 1200 samples
    train: 1200 samples
    umbrella: 1200 samples
    washing machine: 1200 samples
  Per-class loading complete (concurrent): 60000 samples
Total samples: 60000
‚è±Ô∏è  Train dataset creation took: 0.47s
‚è±Ô∏è  Creating val dataset...
Loading QuickDraw dataset from: ../data/quickdraw_parquet
Available classes: 344
Using 50 classes: ['aircraft carrier', 'arm', 'asparagus', 'backpack', 'banana', 'basketball', 'bottlecap', 'bread', 'broom', 'bulldozer']...
Using per-class Parquet format (recommended)...
  Loading 50 classes concurrently from per-class files...
    aircraft carrier: 1200 samples
    arm: 1200 samples
    asparagus: 1200 samples
    backpack: 1200 samples
    banana: 1200 samples
    basketball: 1200 samples
    bottlecap: 1200 samples
    bread: 1200 samples
    broom: 1200 samples
    bulldozer: 1200 samples
    butterfly: 1200 samples
    camel: 1200 samples
    canoe: 1200 samples
    chair: 1200 samples
    compass: 1200 samples
    cookie: 1200 samples
    drums: 1200 samples
    eyeglasses: 1200 samples
    face: 1200 samples
    fan: 1200 samples
    fence: 1200 samples
    fish: 1200 samples
    flying saucer: 1200 samples
    grapes: 1200 samples
    hand: 1200 samples
    hat: 1200 samples
    horse: 1200 samples
    light bulb: 1200 samples
    lighthouse: 1200 samples
    line: 1200 samples
    marker: 1200 samples
    mountain: 1200 samples
    mouse: 1200 samples
    parachute: 1200 samples
    passport: 1200 samples
    pliers: 1200 samples
    potato: 1200 samples
    sea turtle: 1200 samples
    snowflake: 1200 samples
    spider: 1200 samples
    square: 1200 samples
    steak: 1200 samples
    swing set: 1200 samples
    sword: 1200 samples
    television: 1200 samples
    tennis racquet: 1200 samples
    toothbrush: 1200 samples
    train: 1200 samples
    umbrella: 1200 samples
    washing machine: 1200 samples
  Per-class loading complete (concurrent): 60000 samples
Total samples: 60000
‚è±Ô∏è  Val dataset creation took: 0.44s

Loading train/val split...
Using pre-computed split (fast loading)
‚è±Ô∏è  Split loading took: 0.01s
‚è±Ô∏è  Subset creation took: 0.00s
‚è±Ô∏è  DataLoader creation took: 0.00s
‚è±Ô∏è  TOTAL data loading time: 0.02s

Dataloaders created:
  Training: 48 batches (50000 samples)
  Validation: 10 batches (10000 samples)
Dataset: 50 classes, 48 train batches

Building model:
   Architecture: vit_tiny_patch16_224
   Input channels: 1 (grayscale)
   Output classes: 50
Creating vit_tiny_patch16_224 with 50 classes
   Single-channel input: 1 ‚Üí 3 channel conversion
   Pretrained: False
   Drop path rate: 0.1
   Random initialization for single-channel weights

Model info:
   Total parameters: 5,435,762
   Trainable parameters: 5,435,762
   Model size: 20.74 MB
Using 4 GPUs with LAMB optimizer
Setting up deterministic training (seed=42)
Created lamb (fallback to AdamW) optimizer:
   Learning rate: 0.000447
   Weight decay: 0.14103961295141468
   Decay params: 52
   No-decay params: 100
Created learning rate scheduler (step-based):
   Warmup steps: 96
   Total steps: 1440
   Steps per epoch: 48
Created loss function:
   Type: CrossEntropyLoss
   Label smoothing: 0.1
Created AMP gradient scaler
Trainer initialized:
  Model: lamb_lr_0.000447_wd_0.1410
  Train batches: 48
  Val batches: 10
  Save directory: experiments/wandb_lamb_runs
Starting LAMB training for 30 epochs...
/localdrive/users/dkreinov/quickdraw-mobilevit-quant/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:233.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/localdrive/users/dkreinov/quickdraw-mobilevit-quant/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: Memory Efficient attention defaults to a non-deterministic algorithm. To explicitly enable determinism call torch.use_deterministic_algorithms(True, warn_only=False). (Triggered internally at /pytorch/aten/src/ATen/native/transformers/cuda/attention_backward.cu:775.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  Batch    0/48 | Loss: 3.9515 | LR: 9.07e-06 | GradNorm: 2.573
  Batch    4/48 | Loss: 3.9302 | LR: 2.75e-05 | GradNorm: 2.911
  Batch    8/48 | Loss: 3.9182 | LR: 4.59e-05 | GradNorm: 1.759
  Batch   12/48 | Loss: 3.9181 | LR: 6.44e-05 | GradNorm: 2.070
  Batch   16/48 | Loss: 3.9168 | LR: 8.28e-05 | GradNorm: 0.941
  Batch   20/48 | Loss: 3.9123 | LR: 1.01e-04 | GradNorm: 0.674
  Batch   24/48 | Loss: 3.9131 | LR: 1.20e-04 | GradNorm: 0.952
  Batch   28/48 | Loss: 3.9094 | LR: 1.38e-04 | GradNorm: 0.736
  Batch   32/48 | Loss: 3.9030 | LR: 1.56e-04 | GradNorm: 0.699
  Batch   36/48 | Loss: 3.8872 | LR: 1.75e-04 | GradNorm: 0.656
  Batch   40/48 | Loss: 3.8436 | LR: 1.93e-04 | GradNorm: 1.359
  Batch   44/48 | Loss: 3.8813 | LR: 2.12e-04 | GradNorm: 5.771
Epoch 1/30: Train=2.86%, Val=4.15%, Best=4.15% (E1), Gap=-1.29%, LR=2.26e-04
  Batch    0/48 | Loss: 3.7850 | LR: 2.30e-04 | GradNorm: 3.242
  Batch    4/48 | Loss: 3.7711 | LR: 2.49e-04 | GradNorm: 2.796
  Batch    8/48 | Loss: 3.7502 | LR: 2.67e-04 | GradNorm: 3.515
  Batch   12/48 | Loss: 3.7358 | LR: 2.85e-04 | GradNorm: 5.588
  Batch   16/48 | Loss: 3.7164 | LR: 3.04e-04 | GradNorm: 3.117
  Batch   20/48 | Loss: 3.6732 | LR: 3.22e-04 | GradNorm: 1.317
  Batch   24/48 | Loss: 3.6344 | LR: 3.41e-04 | GradNorm: 4.240
  Batch   28/48 | Loss: 3.6125 | LR: 3.59e-04 | GradNorm: 1.686
  Batch   32/48 | Loss: 3.5938 | LR: 3.78e-04 | GradNorm: 2.331
  Batch   36/48 | Loss: 3.6059 | LR: 3.96e-04 | GradNorm: 3.934
  Batch   40/48 | Loss: 3.5774 | LR: 4.14e-04 | GradNorm: 4.088
  Batch   44/48 | Loss: 3.7218 | LR: 4.33e-04 | GradNorm: 10.514
/localdrive/users/dkreinov/quickdraw-mobilevit-quant/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:209: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Epoch 2/30: Train=7.39%, Val=7.91%, Best=7.91% (E2), Gap=-0.52%, LR=4.47e-04
  Batch    0/48 | Loss: 3.8245 | LR: 4.47e-04 | GradNorm: 6.277
  Batch    4/48 | Loss: 3.6244 | LR: 4.47e-04 | GradNorm: 3.576
  Batch    8/48 | Loss: 3.5008 | LR: 4.47e-04 | GradNorm: 4.678
  Batch   12/48 | Loss: 3.8588 | LR: 4.47e-04 | GradNorm: 13.860
  Batch   16/48 | Loss: 3.6026 | LR: 4.47e-04 | GradNorm: 3.849
  Batch   20/48 | Loss: 3.5691 | LR: 4.46e-04 | GradNorm: 4.077
  Batch   24/48 | Loss: 3.6428 | LR: 4.46e-04 | GradNorm: 5.253
  Batch   28/48 | Loss: 3.4929 | LR: 4.46e-04 | GradNorm: 4.733
  Batch   32/48 | Loss: 3.4246 | LR: 4.46e-04 | GradNorm: 1.036
  Batch   36/48 | Loss: 3.4437 | LR: 4.46e-04 | GradNorm: 1.766
  Batch   40/48 | Loss: 3.4754 | LR: 4.46e-04 | GradNorm: 3.225
  Batch   44/48 | Loss: 3.3836 | LR: 4.45e-04 | GradNorm: 2.968
Epoch 3/30: Train=10.31%, Val=8.37%, Best=8.37% (E3), Gap=1.94%, LR=4.45e-04
  Batch    0/48 | Loss: 3.3218 | LR: 4.45e-04 | GradNorm: 2.613
  Batch    4/48 | Loss: 3.3766 | LR: 4.45e-04 | GradNorm: 5.143
  Batch    8/48 | Loss: 3.3263 | LR: 4.45e-04 | GradNorm: 3.061
  Batch   12/48 | Loss: 3.2148 | LR: 4.44e-04 | GradNorm: 2.599
  Batch   16/48 | Loss: 3.2556 | LR: 4.44e-04 | GradNorm: 3.334
  Batch   20/48 | Loss: 3.2212 | LR: 4.44e-04 | GradNorm: 3.868
  Batch   24/48 | Loss: 3.1929 | LR: 4.43e-04 | GradNorm: 4.176
  Batch   28/48 | Loss: 3.1439 | LR: 4.43e-04 | GradNorm: 1.618
  Batch   32/48 | Loss: 3.2591 | LR: 4.43e-04 | GradNorm: 4.709
  Batch   36/48 | Loss: 3.1733 | LR: 4.42e-04 | GradNorm: 2.814
  Batch   40/48 | Loss: 3.1591 | LR: 4.42e-04 | GradNorm: 2.526
  Batch   44/48 | Loss: 3.0908 | LR: 4.41e-04 | GradNorm: 2.911
Epoch 4/30: Train=17.76%, Val=10.01%, Best=10.01% (E4), Gap=7.75%, LR=4.41e-04
  Batch    0/48 | Loss: 3.0917 | LR: 4.41e-04 | GradNorm: 3.026
  Batch    4/48 | Loss: 3.0511 | LR: 4.41e-04 | GradNorm: 3.985
  Batch    8/48 | Loss: 3.0504 | LR: 4.40e-04 | GradNorm: 3.219
  Batch   12/48 | Loss: 3.0362 | LR: 4.40e-04 | GradNorm: 4.808
  Batch   16/48 | Loss: 3.1129 | LR: 4.39e-04 | GradNorm: 5.077
  Batch   20/48 | Loss: 2.9872 | LR: 4.38e-04 | GradNorm: 2.413
  Batch   24/48 | Loss: 3.0466 | LR: 4.38e-04 | GradNorm: 4.067
  Batch   28/48 | Loss: 3.0596 | LR: 4.37e-04 | GradNorm: 5.605
  Batch   32/48 | Loss: 2.9751 | LR: 4.37e-04 | GradNorm: 3.969
  Batch   36/48 | Loss: 2.9875 | LR: 4.36e-04 | GradNorm: 2.243
  Batch   40/48 | Loss: 3.0062 | LR: 4.35e-04 | GradNorm: 3.257
  Batch   44/48 | Loss: 3.0014 | LR: 4.35e-04 | GradNorm: 6.055
Epoch 5/30: Train=23.37%, Val=12.08%, Best=12.08% (E5), Gap=11.29%, LR=4.34e-04
  Batch    0/48 | Loss: 2.9437 | LR: 4.34e-04 | GradNorm: 5.649
  Batch    4/48 | Loss: 2.8378 | LR: 4.33e-04 | GradNorm: 2.281
  Batch    8/48 | Loss: 2.9237 | LR: 4.33e-04 | GradNorm: 3.534
  Batch   12/48 | Loss: 2.9055 | LR: 4.32e-04 | GradNorm: 3.502
  Batch   16/48 | Loss: 2.8364 | LR: 4.31e-04 | GradNorm: 3.077
  Batch   20/48 | Loss: 2.8528 | LR: 4.30e-04 | GradNorm: 3.946
  Batch   24/48 | Loss: 2.9083 | LR: 4.30e-04 | GradNorm: 4.800
  Batch   28/48 | Loss: 2.8578 | LR: 4.29e-04 | GradNorm: 3.889
  Batch   32/48 | Loss: 2.8843 | LR: 4.28e-04 | GradNorm: 5.897
  Batch   36/48 | Loss: 2.8829 | LR: 4.27e-04 | GradNorm: 2.493
  Batch   40/48 | Loss: 2.8992 | LR: 4.26e-04 | GradNorm: 5.515
  Batch   44/48 | Loss: 2.9073 | LR: 4.25e-04 | GradNorm: 4.560
Epoch 6/30: Train=26.77%, Val=18.00%, Best=18.00% (E6), Gap=8.77%, LR=4.25e-04
  Batch    0/48 | Loss: 2.8666 | LR: 4.24e-04 | GradNorm: 7.168
  Batch    4/48 | Loss: 2.8153 | LR: 4.23e-04 | GradNorm: 3.558
  Batch    8/48 | Loss: 2.8105 | LR: 4.23e-04 | GradNorm: 3.367
  Batch   12/48 | Loss: 2.7096 | LR: 4.22e-04 | GradNorm: 2.775
  Batch   16/48 | Loss: 2.7383 | LR: 4.21e-04 | GradNorm: 3.340
  Batch   20/48 | Loss: 2.7672 | LR: 4.20e-04 | GradNorm: 4.875
  Batch   24/48 | Loss: 2.7910 | LR: 4.19e-04 | GradNorm: 5.307
  Batch   28/48 | Loss: 2.7990 | LR: 4.18e-04 | GradNorm: 3.598
  Batch   32/48 | Loss: 2.7873 | LR: 4.17e-04 | GradNorm: 3.440
  Batch   36/48 | Loss: 2.8083 | LR: 4.15e-04 | GradNorm: 4.357
  Batch   40/48 | Loss: 2.8330 | LR: 4.14e-04 | GradNorm: 15.521
  Batch   44/48 | Loss: 2.7596 | LR: 4.13e-04 | GradNorm: 5.299
Epoch 7/30: Train=30.12%, Val=16.88%, Best=18.00% (E6), Gap=13.24%, LR=4.13e-04
  Batch    0/48 | Loss: 2.7539 | LR: 4.12e-04 | GradNorm: 4.306
  Batch    4/48 | Loss: 2.6615 | LR: 4.11e-04 | GradNorm: 2.972
  Batch    8/48 | Loss: 2.6783 | LR: 4.10e-04 | GradNorm: 2.446
  Batch   12/48 | Loss: 2.7191 | LR: 4.09e-04 | GradNorm: 4.996
  Batch   16/48 | Loss: 2.7244 | LR: 4.08e-04 | GradNorm: 3.277
  Batch   20/48 | Loss: 2.6715 | LR: 4.06e-04 | GradNorm: 6.398
  Batch   24/48 | Loss: 2.6645 | LR: 4.05e-04 | GradNorm: 5.674
  Batch   28/48 | Loss: 2.6477 | LR: 4.04e-04 | GradNorm: 4.511
  Batch   32/48 | Loss: 2.6518 | LR: 4.03e-04 | GradNorm: 5.690
  Batch   36/48 | Loss: 2.6423 | LR: 4.02e-04 | GradNorm: 5.448
  Batch   40/48 | Loss: 2.5623 | LR: 4.00e-04 | GradNorm: 5.558
  Batch   44/48 | Loss: 2.5882 | LR: 3.99e-04 | GradNorm: 4.427
Epoch 8/30: Train=34.00%, Val=19.96%, Best=19.96% (E8), Gap=14.04%, LR=3.98e-04
  Batch    0/48 | Loss: 2.5892 | LR: 3.98e-04 | GradNorm: 5.320
  Batch    4/48 | Loss: 2.5617 | LR: 3.96e-04 | GradNorm: 5.078
  Batch    8/48 | Loss: 2.6059 | LR: 3.95e-04 | GradNorm: 6.211
  Batch   12/48 | Loss: 2.5066 | LR: 3.94e-04 | GradNorm: 4.791
  Batch   16/48 | Loss: 2.5371 | LR: 3.92e-04 | GradNorm: 8.555
  Batch   20/48 | Loss: 2.4715 | LR: 3.91e-04 | GradNorm: 5.910
  Batch   24/48 | Loss: 2.4865 | LR: 3.90e-04 | GradNorm: 3.559
  Batch   28/48 | Loss: 2.5233 | LR: 3.88e-04 | GradNorm: 5.489
  Batch   32/48 | Loss: 2.4707 | LR: 3.87e-04 | GradNorm: 5.537
  Batch   36/48 | Loss: 2.4404 | LR: 3.85e-04 | GradNorm: 3.266
  Batch   40/48 | Loss: 2.4472 | LR: 3.84e-04 | GradNorm: 5.079
  Batch   44/48 | Loss: 2.4596 | LR: 3.82e-04 | GradNorm: 3.969
Epoch 9/30: Train=39.76%, Val=23.51%, Best=23.51% (E9), Gap=16.25%, LR=3.81e-04
  Batch    0/48 | Loss: 2.4564 | LR: 3.81e-04 | GradNorm: 5.365
  Batch    4/48 | Loss: 2.4891 | LR: 3.79e-04 | GradNorm: 6.698
  Batch    8/48 | Loss: 2.3830 | LR: 3.78e-04 | GradNorm: 4.713
  Batch   12/48 | Loss: 2.3665 | LR: 3.76e-04 | GradNorm: 4.292
  Batch   16/48 | Loss: 2.3979 | LR: 3.75e-04 | GradNorm: 3.591
  Batch   20/48 | Loss: 2.3992 | LR: 3.73e-04 | GradNorm: 5.050
  Batch   24/48 | Loss: 2.4120 | LR: 3.72e-04 | GradNorm: 3.803
  Batch   28/48 | Loss: 2.4356 | LR: 3.70e-04 | GradNorm: 6.731
  Batch   32/48 | Loss: 2.4180 | LR: 3.69e-04 | GradNorm: 5.490
  Batch   36/48 | Loss: 2.3274 | LR: 3.67e-04 | GradNorm: 3.185
  Batch   40/48 | Loss: 2.3550 | LR: 3.65e-04 | GradNorm: 5.779
  Batch   44/48 | Loss: 2.3056 | LR: 3.64e-04 | GradNorm: 4.897
Epoch 10/30: Train=42.90%, Val=24.92%, Best=24.92% (E10), Gap=17.98%, LR=3.63e-04
  Batch    0/48 | Loss: 2.3122 | LR: 3.62e-04 | GradNorm: 6.576
  Batch    4/48 | Loss: 2.3031 | LR: 3.61e-04 | GradNorm: 3.980
  Batch    8/48 | Loss: 2.3218 | LR: 3.59e-04 | GradNorm: 5.380
  Batch   12/48 | Loss: 2.2682 | LR: 3.57e-04 | GradNorm: 3.838
  Batch   16/48 | Loss: 2.1981 | LR: 3.56e-04 | GradNorm: 3.913
  Batch   20/48 | Loss: 2.3114 | LR: 3.54e-04 | GradNorm: 5.149
  Batch   24/48 | Loss: 2.4068 | LR: 3.52e-04 | GradNorm: 7.205
  Batch   28/48 | Loss: 2.2211 | LR: 3.51e-04 | GradNorm: 7.295
  Batch   32/48 | Loss: 2.3257 | LR: 3.49e-04 | GradNorm: 7.637
  Batch   36/48 | Loss: 2.2693 | LR: 3.47e-04 | GradNorm: 3.869
  Batch   40/48 | Loss: 2.2523 | LR: 3.45e-04 | GradNorm: 3.105
  Batch   44/48 | Loss: 2.2529 | LR: 3.44e-04 | GradNorm: 3.556
Epoch 11/30: Train=46.85%, Val=26.74%, Best=26.74% (E11), Gap=20.11%, LR=3.42e-04
  Batch    0/48 | Loss: 2.2538 | LR: 3.42e-04 | GradNorm: 5.837
  Batch    4/48 | Loss: 2.3420 | LR: 3.40e-04 | GradNorm: 6.389
  Batch    8/48 | Loss: 2.2309 | LR: 3.38e-04 | GradNorm: 4.761
  Batch   12/48 | Loss: 2.2580 | LR: 3.36e-04 | GradNorm: 5.067
  Batch   16/48 | Loss: 2.2301 | LR: 3.35e-04 | GradNorm: 5.141
  Batch   20/48 | Loss: 2.2701 | LR: 3.33e-04 | GradNorm: 6.200
  Batch   24/48 | Loss: 2.2832 | LR: 3.31e-04 | GradNorm: 6.706
  Batch   28/48 | Loss: 2.2123 | LR: 3.29e-04 | GradNorm: 4.530
  Batch   32/48 | Loss: 2.2408 | LR: 3.27e-04 | GradNorm: 6.681
  Batch   36/48 | Loss: 2.1125 | LR: 3.25e-04 | GradNorm: 4.397
  Batch   40/48 | Loss: 2.1474 | LR: 3.24e-04 | GradNorm: 5.254
  Batch   44/48 | Loss: 2.1594 | LR: 3.22e-04 | GradNorm: 6.845
‚ö†Ô∏è  Below trajectory threshold at epoch 12: 34.89% < 80.0% (Count: 1/3)
Epoch 12/30: Train=49.27%, Val=34.89%, Best=34.89% (E12), Gap=14.38%, LR=3.20e-04
  Batch    0/48 | Loss: 2.1995 | LR: 3.20e-04 | GradNorm: 5.942
  Batch    4/48 | Loss: 2.1343 | LR: 3.18e-04 | GradNorm: 4.468
  Batch    8/48 | Loss: 2.0334 | LR: 3.16e-04 | GradNorm: 3.700
  Batch   12/48 | Loss: 2.0645 | LR: 3.14e-04 | GradNorm: 6.237
  Batch   16/48 | Loss: 2.1342 | LR: 3.12e-04 | GradNorm: 6.045
  Batch   20/48 | Loss: 2.0940 | LR: 3.10e-04 | GradNorm: 5.835
  Batch   24/48 | Loss: 2.1672 | LR: 3.08e-04 | GradNorm: 4.493
  Batch   28/48 | Loss: 2.1325 | LR: 3.06e-04 | GradNorm: 5.769
  Batch   32/48 | Loss: 2.0872 | LR: 3.05e-04 | GradNorm: 4.526
  Batch   36/48 | Loss: 2.0957 | LR: 3.03e-04 | GradNorm: 4.644
  Batch   40/48 | Loss: 2.1519 | LR: 3.01e-04 | GradNorm: 6.289
  Batch   44/48 | Loss: 2.0627 | LR: 2.99e-04 | GradNorm: 4.703
‚ö†Ô∏è  Below trajectory threshold at epoch 13: 36.50% < 80.0% (Count: 2/3)
Epoch 13/30: Train=52.31%, Val=36.50%, Best=36.50% (E13), Gap=15.81%, LR=2.97e-04
  Batch    0/48 | Loss: 2.1485 | LR: 2.97e-04 | GradNorm: 4.928
  Batch    4/48 | Loss: 2.1096 | LR: 2.95e-04 | GradNorm: 6.558
  Batch    8/48 | Loss: 2.1209 | LR: 2.93e-04 | GradNorm: 5.256
  Batch   12/48 | Loss: 2.0722 | LR: 2.91e-04 | GradNorm: 4.368
  Batch   16/48 | Loss: 2.0789 | LR: 2.89e-04 | GradNorm: 6.060
  Batch   20/48 | Loss: 2.0081 | LR: 2.87e-04 | GradNorm: 6.850
  Batch   24/48 | Loss: 2.0758 | LR: 2.85e-04 | GradNorm: 4.264
  Batch   28/48 | Loss: 2.0573 | LR: 2.83e-04 | GradNorm: 5.095
  Batch   32/48 | Loss: 2.0391 | LR: 2.81e-04 | GradNorm: 6.085
  Batch   36/48 | Loss: 2.0038 | LR: 2.79e-04 | GradNorm: 5.593
  Batch   40/48 | Loss: 2.0345 | LR: 2.77e-04 | GradNorm: 5.877
  Batch   44/48 | Loss: 2.0014 | LR: 2.75e-04 | GradNorm: 5.457
‚ö†Ô∏è  Below trajectory threshold at epoch 14: 39.57% < 80.0% (Count: 3/3)
üõë Early stopping: Not following successful LAMB trajectory
   Expected: >80.0% by epoch 12, Got: 39.57%
Final best validation accuracy: 39.57% (epoch 14)
