=== LAMB Sweep Run ===
LR: 0.000300
Weight Decay: 0.0110
Warmup Epochs: 5
Using device: cuda
Available GPUs: 4
⏱️  Creating train dataset...
Loading QuickDraw dataset from: ../data/quickdraw_parquet
Available classes: 344
Using 50 classes: ['aircraft carrier', 'arm', 'asparagus', 'backpack', 'banana', 'basketball', 'bottlecap', 'bread', 'broom', 'bulldozer']...
Using per-class Parquet format (recommended)...
  Loading 50 classes concurrently from per-class files...
    aircraft carrier: 1200 samples
    arm: 1200 samples
    asparagus: 1200 samples
    backpack: 1200 samples
    banana: 1200 samples
    basketball: 1200 samples
    bottlecap: 1200 samples
    bread: 1200 samples
    broom: 1200 samples
    bulldozer: 1200 samples
    butterfly: 1200 samples
    camel: 1200 samples
    canoe: 1200 samples
    chair: 1200 samples
    compass: 1200 samples
    cookie: 1200 samples
    drums: 1200 samples
    eyeglasses: 1200 samples
    face: 1200 samples
    fan: 1200 samples
    fence: 1200 samples
    fish: 1200 samples
    flying saucer: 1200 samples
    grapes: 1200 samples
    hand: 1200 samples
    hat: 1200 samples
    horse: 1200 samples
    light bulb: 1200 samples
    lighthouse: 1200 samples
    line: 1200 samples
    marker: 1200 samples
    mountain: 1200 samples
    mouse: 1200 samples
    parachute: 1200 samples
    passport: 1200 samples
    pliers: 1200 samples
    potato: 1200 samples
    sea turtle: 1200 samples
    snowflake: 1200 samples
    spider: 1200 samples
    square: 1200 samples
    steak: 1200 samples
    swing set: 1200 samples
    sword: 1200 samples
    television: 1200 samples
    tennis racquet: 1200 samples
    toothbrush: 1200 samples
    train: 1200 samples
    umbrella: 1200 samples
    washing machine: 1200 samples
  Per-class loading complete (concurrent): 60000 samples
Total samples: 60000
⏱️  Train dataset creation took: 0.47s
⏱️  Creating val dataset...
Loading QuickDraw dataset from: ../data/quickdraw_parquet
Available classes: 344
Using 50 classes: ['aircraft carrier', 'arm', 'asparagus', 'backpack', 'banana', 'basketball', 'bottlecap', 'bread', 'broom', 'bulldozer']...
Using per-class Parquet format (recommended)...
  Loading 50 classes concurrently from per-class files...
    aircraft carrier: 1200 samples
    arm: 1200 samples
    asparagus: 1200 samples
    backpack: 1200 samples
    banana: 1200 samples
    basketball: 1200 samples
    bottlecap: 1200 samples
    bread: 1200 samples
    broom: 1200 samples
    bulldozer: 1200 samples
    butterfly: 1200 samples
    camel: 1200 samples
    canoe: 1200 samples
    chair: 1200 samples
    compass: 1200 samples
    cookie: 1200 samples
    drums: 1200 samples
    eyeglasses: 1200 samples
    face: 1200 samples
    fan: 1200 samples
    fence: 1200 samples
    fish: 1200 samples
    flying saucer: 1200 samples
    grapes: 1200 samples
    hand: 1200 samples
    hat: 1200 samples
    horse: 1200 samples
    light bulb: 1200 samples
    lighthouse: 1200 samples
    line: 1200 samples
    marker: 1200 samples
    mountain: 1200 samples
    mouse: 1200 samples
    parachute: 1200 samples
    passport: 1200 samples
    pliers: 1200 samples
    potato: 1200 samples
    sea turtle: 1200 samples
    snowflake: 1200 samples
    spider: 1200 samples
    square: 1200 samples
    steak: 1200 samples
    swing set: 1200 samples
    sword: 1200 samples
    television: 1200 samples
    tennis racquet: 1200 samples
    toothbrush: 1200 samples
    train: 1200 samples
    umbrella: 1200 samples
    washing machine: 1200 samples
  Per-class loading complete (concurrent): 60000 samples
Total samples: 60000
⏱️  Val dataset creation took: 0.44s

Loading train/val split...
Using pre-computed split (fast loading)
⏱️  Split loading took: 0.01s
⏱️  Subset creation took: 0.00s
⏱️  DataLoader creation took: 0.00s
⏱️  TOTAL data loading time: 0.02s

Dataloaders created:
  Training: 97 batches (50000 samples)
  Validation: 20 batches (10000 samples)
Dataset: 50 classes, 97 train batches

Building model:
   Architecture: vit_tiny_patch16_224
   Input channels: 1 (grayscale)
   Output classes: 50
Creating vit_tiny_patch16_224 with 50 classes
   Single-channel input: 1 → 3 channel conversion
   Pretrained: False
   Drop path rate: 0.1
   Random initialization for single-channel weights

Model info:
   Total parameters: 5,435,762
   Trainable parameters: 5,435,762
   Model size: 20.74 MB
Using 4 GPUs with LAMB optimizer
Setting up deterministic training (seed=42)
Created lamb (fallback to AdamW) optimizer:
   Learning rate: 0.000300
   Weight decay: 0.010991438613633712
   Decay params: 52
   No-decay params: 100
Created learning rate scheduler (step-based):
   Warmup steps: 485
   Total steps: 2910
   Steps per epoch: 97
Created loss function:
   Type: CrossEntropyLoss
   Label smoothing: 0.0763076666069005
Created AMP gradient scaler
Trainer initialized:
  Model: lamb_lr_0.000300_wd_0.0110
  Train batches: 97
  Val batches: 20
  Save directory: experiments/wandb_lamb_runs
Starting LAMB training for 30 epochs...
/localdrive/users/dkreinov/quickdraw-mobilevit-quant/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:233.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/localdrive/users/dkreinov/quickdraw-mobilevit-quant/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: Memory Efficient attention defaults to a non-deterministic algorithm. To explicitly enable determinism call torch.use_deterministic_algorithms(True, warn_only=False). (Triggered internally at /pytorch/aten/src/ATen/native/transformers/cuda/attention_backward.cu:775.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  Batch    0/97 | Loss: 3.9436 | LR: 3.61e-06 | GradNorm: 1.742
  Batch    9/97 | Loss: 3.9236 | LR: 9.12e-06 | GradNorm: 1.260
  Batch   18/97 | Loss: 3.9258 | LR: 1.46e-05 | GradNorm: 1.370
  Batch   27/97 | Loss: 3.9096 | LR: 2.01e-05 | GradNorm: 1.079
  Batch   36/97 | Loss: 3.9091 | LR: 2.57e-05 | GradNorm: 0.896
  Batch   45/97 | Loss: 3.9066 | LR: 3.12e-05 | GradNorm: 1.032
  Batch   54/97 | Loss: 3.9049 | LR: 3.67e-05 | GradNorm: 0.940
  Batch   63/97 | Loss: 3.9083 | LR: 4.22e-05 | GradNorm: 1.195
  Batch   72/97 | Loss: 3.8864 | LR: 4.77e-05 | GradNorm: 2.488
  Batch   81/97 | Loss: 3.8156 | LR: 5.32e-05 | GradNorm: 2.071
  Batch   90/97 | Loss: 3.7710 | LR: 5.87e-05 | GradNorm: 3.381
Epoch 1/30: Train=3.20%, Val=3.06%, Best=3.06% (E1), Gap=0.14%, LR=6.24e-05
  Batch    0/97 | Loss: 3.8235 | LR: 6.30e-05 | GradNorm: 10.014
  Batch    9/97 | Loss: 3.7212 | LR: 6.85e-05 | GradNorm: 1.527
  Batch   18/97 | Loss: 3.6777 | LR: 7.40e-05 | GradNorm: 6.862
  Batch   27/97 | Loss: 3.6452 | LR: 7.95e-05 | GradNorm: 3.998
  Batch   36/97 | Loss: 3.6482 | LR: 8.51e-05 | GradNorm: 12.132
  Batch   45/97 | Loss: 3.6211 | LR: 9.06e-05 | GradNorm: 9.321
  Batch   54/97 | Loss: 3.5925 | LR: 9.61e-05 | GradNorm: 11.062
  Batch   63/97 | Loss: 3.5695 | LR: 1.02e-04 | GradNorm: 8.215
  Batch   72/97 | Loss: 3.4944 | LR: 1.07e-04 | GradNorm: 5.015
  Batch   81/97 | Loss: 3.4925 | LR: 1.13e-04 | GradNorm: 4.183
  Batch   90/97 | Loss: 3.4152 | LR: 1.18e-04 | GradNorm: 8.496
Epoch 2/30: Train=9.72%, Val=13.16%, Best=13.16% (E2), Gap=-3.44%, LR=1.22e-04
  Batch    0/97 | Loss: 3.3573 | LR: 1.22e-04 | GradNorm: 3.475
  Batch    9/97 | Loss: 3.5299 | LR: 1.28e-04 | GradNorm: 12.218
  Batch   18/97 | Loss: 3.3928 | LR: 1.33e-04 | GradNorm: 9.213
  Batch   27/97 | Loss: 3.3211 | LR: 1.39e-04 | GradNorm: 4.378
  Batch   36/97 | Loss: 3.2934 | LR: 1.44e-04 | GradNorm: 4.960
  Batch   45/97 | Loss: 3.2905 | LR: 1.50e-04 | GradNorm: 5.386
  Batch   54/97 | Loss: 3.2975 | LR: 1.55e-04 | GradNorm: 5.379
  Batch   63/97 | Loss: 3.2587 | LR: 1.61e-04 | GradNorm: 8.228
  Batch   72/97 | Loss: 3.2591 | LR: 1.67e-04 | GradNorm: 5.997
  Batch   81/97 | Loss: 3.2584 | LR: 1.72e-04 | GradNorm: 6.785
  Batch   90/97 | Loss: 3.2134 | LR: 1.78e-04 | GradNorm: 8.450
Epoch 3/30: Train=16.15%, Val=11.59%, Best=13.16% (E2), Gap=4.56%, LR=1.81e-04
  Batch    0/97 | Loss: 3.1825 | LR: 1.82e-04 | GradNorm: 6.041
  Batch    9/97 | Loss: 3.1746 | LR: 1.87e-04 | GradNorm: 5.950
