=== LAMB Sweep Run ===
LR: 0.000300
Weight Decay: 0.0500
Warmup Epochs: 3
Using device: cuda
Available GPUs: 4
⏱️  Creating train dataset...
Loading QuickDraw dataset from: ../data/quickdraw_parquet
Available classes: 344
Using 50 classes: ['aircraft carrier', 'arm', 'asparagus', 'backpack', 'banana', 'basketball', 'bottlecap', 'bread', 'broom', 'bulldozer']...
Using per-class Parquet format (recommended)...
  Loading 50 classes concurrently from per-class files...
    aircraft carrier: 1200 samples
    arm: 1200 samples
    asparagus: 1200 samples
    backpack: 1200 samples
    banana: 1200 samples
    basketball: 1200 samples
    bottlecap: 1200 samples
    bread: 1200 samples
    broom: 1200 samples
    bulldozer: 1200 samples
    butterfly: 1200 samples
    camel: 1200 samples
    canoe: 1200 samples
    chair: 1200 samples
    compass: 1200 samples
    cookie: 1200 samples
    drums: 1200 samples
    eyeglasses: 1200 samples
    face: 1200 samples
    fan: 1200 samples
    fence: 1200 samples
    fish: 1200 samples
    flying saucer: 1200 samples
    grapes: 1200 samples
    hand: 1200 samples
    hat: 1200 samples
    horse: 1200 samples
    light bulb: 1200 samples
    lighthouse: 1200 samples
    line: 1200 samples
    marker: 1200 samples
    mountain: 1200 samples
    mouse: 1200 samples
    parachute: 1200 samples
    passport: 1200 samples
    pliers: 1200 samples
    potato: 1200 samples
    sea turtle: 1200 samples
    snowflake: 1200 samples
    spider: 1200 samples
    square: 1200 samples
    steak: 1200 samples
    swing set: 1200 samples
    sword: 1200 samples
    television: 1200 samples
    tennis racquet: 1200 samples
    toothbrush: 1200 samples
    train: 1200 samples
    umbrella: 1200 samples
    washing machine: 1200 samples
  Per-class loading complete (concurrent): 60000 samples
Total samples: 60000
⏱️  Train dataset creation took: 0.47s
⏱️  Creating val dataset...
Loading QuickDraw dataset from: ../data/quickdraw_parquet
Available classes: 344
Using 50 classes: ['aircraft carrier', 'arm', 'asparagus', 'backpack', 'banana', 'basketball', 'bottlecap', 'bread', 'broom', 'bulldozer']...
Using per-class Parquet format (recommended)...
  Loading 50 classes concurrently from per-class files...
    aircraft carrier: 1200 samples
    arm: 1200 samples
    asparagus: 1200 samples
    backpack: 1200 samples
    banana: 1200 samples
    basketball: 1200 samples
    bottlecap: 1200 samples
    bread: 1200 samples
    broom: 1200 samples
    bulldozer: 1200 samples
    butterfly: 1200 samples
    camel: 1200 samples
    canoe: 1200 samples
    chair: 1200 samples
    compass: 1200 samples
    cookie: 1200 samples
    drums: 1200 samples
    eyeglasses: 1200 samples
    face: 1200 samples
    fan: 1200 samples
    fence: 1200 samples
    fish: 1200 samples
    flying saucer: 1200 samples
    grapes: 1200 samples
    hand: 1200 samples
    hat: 1200 samples
    horse: 1200 samples
    light bulb: 1200 samples
    lighthouse: 1200 samples
    line: 1200 samples
    marker: 1200 samples
    mountain: 1200 samples
    mouse: 1200 samples
    parachute: 1200 samples
    passport: 1200 samples
    pliers: 1200 samples
    potato: 1200 samples
    sea turtle: 1200 samples
    snowflake: 1200 samples
    spider: 1200 samples
    square: 1200 samples
    steak: 1200 samples
    swing set: 1200 samples
    sword: 1200 samples
    television: 1200 samples
    tennis racquet: 1200 samples
    toothbrush: 1200 samples
    train: 1200 samples
    umbrella: 1200 samples
    washing machine: 1200 samples
  Per-class loading complete (concurrent): 60000 samples
Total samples: 60000
⏱️  Val dataset creation took: 0.43s

Loading train/val split...
Using pre-computed split (fast loading)
⏱️  Split loading took: 0.01s
⏱️  Subset creation took: 0.00s
⏱️  DataLoader creation took: 0.00s
⏱️  TOTAL data loading time: 0.02s

Dataloaders created:
  Training: 48 batches (50000 samples)
  Validation: 10 batches (10000 samples)
Dataset: 50 classes, 48 train batches

Building model:
   Architecture: vit_tiny_patch16_224
   Input channels: 1 (grayscale)
   Output classes: 50
Creating vit_tiny_patch16_224 with 50 classes
   Single-channel input: 1 → 3 channel conversion
   Pretrained: False
   Drop path rate: 0.1
   Random initialization for single-channel weights

Model info:
   Total parameters: 5,435,762
   Trainable parameters: 5,435,762
   Model size: 20.74 MB
Using 4 GPUs with LAMB optimizer
🔍 Debug Info:
   Optimizer requested: lamb
   ✅ LAMB import successful in wandb script
Setting up deterministic training (seed=42)
   TrainingConfig optimizer_name: lamb
   CUSTOM_OPTIMIZERS_AVAILABLE in TrainingConfig: True
Created LAMB optimizer:
   Learning rate: 0.000300
   Weight decay: 0.05
   Optimizer type: LAMB
Created learning rate scheduler (step-based):
   Warmup steps: 144
   Total steps: 1440
   Steps per epoch: 48
Created loss function:
   Type: CrossEntropyLoss
   Label smoothing: 0.1
Created AMP gradient scaler
Trainer initialized:
  Model: lamb_lr_0.000300_wd_0.0500
  Train batches: 48
  Val batches: 10
  Save directory: experiments/wandb_lamb_runs
Starting LAMB training for 30 epochs...
/localdrive/users/dkreinov/quickdraw-mobilevit-quant/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:233.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/localdrive/users/dkreinov/quickdraw-mobilevit-quant/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: Memory Efficient attention defaults to a non-deterministic algorithm. To explicitly enable determinism call torch.use_deterministic_algorithms(True, warn_only=False). (Triggered internally at /pytorch/aten/src/ATen/native/transformers/cuda/attention_backward.cu:775.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  Batch    0/48 | Loss: 3.9460 | LR: 5.06e-06 | GradNorm: 1.563
  Batch    4/48 | Loss: 3.9459 | LR: 1.33e-05 | GradNorm: 1.302
  Batch    8/48 | Loss: 3.9433 | LR: 2.16e-05 | GradNorm: 1.331
  Batch   12/48 | Loss: 3.9541 | LR: 2.98e-05 | GradNorm: 1.705
  Batch   16/48 | Loss: 3.9524 | LR: 3.81e-05 | GradNorm: 1.337
  Batch   20/48 | Loss: 3.9388 | LR: 4.63e-05 | GradNorm: 1.302
  Batch   24/48 | Loss: 3.9318 | LR: 5.46e-05 | GradNorm: 1.334
  Batch   28/48 | Loss: 3.9387 | LR: 6.28e-05 | GradNorm: 1.206
  Batch   32/48 | Loss: 3.9301 | LR: 7.11e-05 | GradNorm: 1.046
  Batch   36/48 | Loss: 3.9266 | LR: 7.93e-05 | GradNorm: 1.009
  Batch   40/48 | Loss: 3.9162 | LR: 8.76e-05 | GradNorm: 0.929
  Batch   44/48 | Loss: 3.9183 | LR: 9.58e-05 | GradNorm: 0.930
Epoch 1/30: Train=2.14%, Val=2.86%, Best=2.86% (E1), Gap=-0.72%, LR=1.02e-04
  Batch    0/48 | Loss: 3.9161 | LR: 1.04e-04 | GradNorm: 0.912
  Batch    4/48 | Loss: 3.9113 | LR: 1.12e-04 | GradNorm: 0.700
  Batch    8/48 | Loss: 3.9167 | LR: 1.21e-04 | GradNorm: 0.877
  Batch   12/48 | Loss: 3.9117 | LR: 1.29e-04 | GradNorm: 0.832
  Batch   16/48 | Loss: 3.9135 | LR: 1.37e-04 | GradNorm: 0.777
  Batch   20/48 | Loss: 3.9151 | LR: 1.45e-04 | GradNorm: 0.767
  Batch   24/48 | Loss: 3.9130 | LR: 1.54e-04 | GradNorm: 0.810
  Batch   28/48 | Loss: 3.9180 | LR: 1.62e-04 | GradNorm: 0.929
  Batch   32/48 | Loss: 3.9079 | LR: 1.70e-04 | GradNorm: 0.647
  Batch   36/48 | Loss: 3.9095 | LR: 1.78e-04 | GradNorm: 0.873
  Batch   40/48 | Loss: 3.9104 | LR: 1.87e-04 | GradNorm: 0.724
  Batch   44/48 | Loss: 3.9065 | LR: 1.95e-04 | GradNorm: 0.684
Epoch 2/30: Train=2.36%, Val=2.99%, Best=2.99% (E2), Gap=-0.63%, LR=2.01e-04
  Batch    0/48 | Loss: 3.9067 | LR: 2.03e-04 | GradNorm: 0.942
  Batch    4/48 | Loss: 3.9042 | LR: 2.11e-04 | GradNorm: 0.753
  Batch    8/48 | Loss: 3.8995 | LR: 2.20e-04 | GradNorm: 0.723
  Batch   12/48 | Loss: 3.8998 | LR: 2.28e-04 | GradNorm: 0.769
  Batch   16/48 | Loss: 3.8974 | LR: 2.36e-04 | GradNorm: 0.883
