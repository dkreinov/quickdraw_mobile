=== LAMB Sweep Run ===
LR: 0.000300
Weight Decay: 0.1135
Warmup Epochs: 7
Using device: cuda
Available GPUs: 4
⏱️  Creating train dataset...
Loading QuickDraw dataset from: ../data/quickdraw_parquet
Available classes: 344
Using 50 classes: ['aircraft carrier', 'arm', 'asparagus', 'backpack', 'banana', 'basketball', 'bottlecap', 'bread', 'broom', 'bulldozer']...
Using per-class Parquet format (recommended)...
  Loading 50 classes concurrently from per-class files...
    aircraft carrier: 1200 samples
    arm: 1200 samples
    asparagus: 1200 samples
    backpack: 1200 samples
    banana: 1200 samples
    basketball: 1200 samples
    bottlecap: 1200 samples
    bread: 1200 samples
    broom: 1200 samples
    bulldozer: 1200 samples
    butterfly: 1200 samples
    camel: 1200 samples
    canoe: 1200 samples
    chair: 1200 samples
    compass: 1200 samples
    cookie: 1200 samples
    drums: 1200 samples
    eyeglasses: 1200 samples
    face: 1200 samples
    fan: 1200 samples
    fence: 1200 samples
    fish: 1200 samples
    flying saucer: 1200 samples
    grapes: 1200 samples
    hand: 1200 samples
    hat: 1200 samples
    horse: 1200 samples
    light bulb: 1200 samples
    lighthouse: 1200 samples
    line: 1200 samples
    marker: 1200 samples
    mountain: 1200 samples
    mouse: 1200 samples
    parachute: 1200 samples
    passport: 1200 samples
    pliers: 1200 samples
    potato: 1200 samples
    sea turtle: 1200 samples
    snowflake: 1200 samples
    spider: 1200 samples
    square: 1200 samples
    steak: 1200 samples
    swing set: 1200 samples
    sword: 1200 samples
    television: 1200 samples
    tennis racquet: 1200 samples
    toothbrush: 1200 samples
    train: 1200 samples
    umbrella: 1200 samples
    washing machine: 1200 samples
  Per-class loading complete (concurrent): 60000 samples
Total samples: 60000
⏱️  Train dataset creation took: 0.48s
⏱️  Creating val dataset...
Loading QuickDraw dataset from: ../data/quickdraw_parquet
Available classes: 344
Using 50 classes: ['aircraft carrier', 'arm', 'asparagus', 'backpack', 'banana', 'basketball', 'bottlecap', 'bread', 'broom', 'bulldozer']...
Using per-class Parquet format (recommended)...
  Loading 50 classes concurrently from per-class files...
    aircraft carrier: 1200 samples
    arm: 1200 samples
    asparagus: 1200 samples
    backpack: 1200 samples
    banana: 1200 samples
    basketball: 1200 samples
    bottlecap: 1200 samples
    bread: 1200 samples
    broom: 1200 samples
    bulldozer: 1200 samples
    butterfly: 1200 samples
    camel: 1200 samples
    canoe: 1200 samples
    chair: 1200 samples
    compass: 1200 samples
    cookie: 1200 samples
    drums: 1200 samples
    eyeglasses: 1200 samples
    face: 1200 samples
    fan: 1200 samples
    fence: 1200 samples
    fish: 1200 samples
    flying saucer: 1200 samples
    grapes: 1200 samples
    hand: 1200 samples
    hat: 1200 samples
    horse: 1200 samples
    light bulb: 1200 samples
    lighthouse: 1200 samples
    line: 1200 samples
    marker: 1200 samples
    mountain: 1200 samples
    mouse: 1200 samples
    parachute: 1200 samples
    passport: 1200 samples
    pliers: 1200 samples
    potato: 1200 samples
    sea turtle: 1200 samples
    snowflake: 1200 samples
    spider: 1200 samples
    square: 1200 samples
    steak: 1200 samples
    swing set: 1200 samples
    sword: 1200 samples
    television: 1200 samples
    tennis racquet: 1200 samples
    toothbrush: 1200 samples
    train: 1200 samples
    umbrella: 1200 samples
    washing machine: 1200 samples
  Per-class loading complete (concurrent): 60000 samples
Total samples: 60000
⏱️  Val dataset creation took: 0.44s

Loading train/val split...
Using pre-computed split (fast loading)
⏱️  Split loading took: 0.01s
⏱️  Subset creation took: 0.00s
⏱️  DataLoader creation took: 0.00s
⏱️  TOTAL data loading time: 0.02s

Dataloaders created:
  Training: 48 batches (50000 samples)
  Validation: 10 batches (10000 samples)
Dataset: 50 classes, 48 train batches

Building model:
   Architecture: vit_tiny_patch16_224
   Input channels: 1 (grayscale)
   Output classes: 50
Creating vit_tiny_patch16_224 with 50 classes
   Single-channel input: 1 → 3 channel conversion
   Pretrained: False
   Drop path rate: 0.1
   Random initialization for single-channel weights

Model info:
   Total parameters: 5,435,762
   Trainable parameters: 5,435,762
   Model size: 20.74 MB
Using 4 GPUs with LAMB optimizer
Setting up deterministic training (seed=42)
Created lamb (fallback to AdamW) optimizer:
   Learning rate: 0.000300
   Weight decay: 0.1135199207461482
   Decay params: 52
   No-decay params: 100
Created learning rate scheduler (step-based):
   Warmup steps: 336
   Total steps: 1440
   Steps per epoch: 48
Created loss function:
   Type: CrossEntropyLoss
   Label smoothing: 0.1087042519792431
Created AMP gradient scaler
Trainer initialized:
  Model: lamb_lr_0.000300_wd_0.1135
  Train batches: 48
  Val batches: 10
  Save directory: experiments/wandb_lamb_runs
Starting LAMB training for 30 epochs...
/localdrive/users/dkreinov/quickdraw-mobilevit-quant/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:233.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/localdrive/users/dkreinov/quickdraw-mobilevit-quant/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: Memory Efficient attention defaults to a non-deterministic algorithm. To explicitly enable determinism call torch.use_deterministic_algorithms(True, warn_only=False). (Triggered internally at /pytorch/aten/src/ATen/native/transformers/cuda/attention_backward.cu:775.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  Batch    0/48 | Loss: 3.9570 | LR: 3.88e-06 | GradNorm: 1.682
  Batch    4/48 | Loss: 3.9430 | LR: 7.42e-06 | GradNorm: 1.228
  Batch    8/48 | Loss: 3.9268 | LR: 1.10e-05 | GradNorm: 0.900
  Batch   12/48 | Loss: 3.9114 | LR: 1.45e-05 | GradNorm: 0.729
  Batch   16/48 | Loss: 3.9202 | LR: 1.80e-05 | GradNorm: 0.833
  Batch   20/48 | Loss: 3.9095 | LR: 2.16e-05 | GradNorm: 0.751
  Batch   24/48 | Loss: 3.9141 | LR: 2.51e-05 | GradNorm: 0.780
  Batch   28/48 | Loss: 3.9171 | LR: 2.86e-05 | GradNorm: 0.724
  Batch   32/48 | Loss: 3.9095 | LR: 3.22e-05 | GradNorm: 0.642
  Batch   36/48 | Loss: 3.9146 | LR: 3.57e-05 | GradNorm: 0.725
  Batch   40/48 | Loss: 3.9111 | LR: 3.92e-05 | GradNorm: 0.686
  Batch   44/48 | Loss: 3.9086 | LR: 4.28e-05 | GradNorm: 0.739
Epoch 1/30: Train=2.26%, Val=5.03%, Best=5.03% (E1), Gap=-2.77%, LR=4.54e-05
  Batch    0/48 | Loss: 3.9124 | LR: 4.63e-05 | GradNorm: 0.770
  Batch    4/48 | Loss: 3.8975 | LR: 4.98e-05 | GradNorm: 0.586
  Batch    8/48 | Loss: 3.8906 | LR: 5.34e-05 | GradNorm: 0.734
  Batch   12/48 | Loss: 3.8608 | LR: 5.69e-05 | GradNorm: 1.548
  Batch   16/48 | Loss: 3.8455 | LR: 6.05e-05 | GradNorm: 2.554
  Batch   20/48 | Loss: 3.8289 | LR: 6.40e-05 | GradNorm: 5.863
  Batch   24/48 | Loss: 3.8133 | LR: 6.75e-05 | GradNorm: 3.401
  Batch   28/48 | Loss: 3.8214 | LR: 7.11e-05 | GradNorm: 7.952
  Batch   32/48 | Loss: 3.8498 | LR: 7.46e-05 | GradNorm: 8.417
  Batch   36/48 | Loss: 3.8071 | LR: 7.81e-05 | GradNorm: 8.168
  Batch   40/48 | Loss: 3.8209 | LR: 8.17e-05 | GradNorm: 6.735
  Batch   44/48 | Loss: 3.8046 | LR: 8.52e-05 | GradNorm: 6.773
Epoch 2/30: Train=4.35%, Val=4.60%, Best=5.03% (E1), Gap=-0.25%, LR=8.79e-05
  Batch    0/48 | Loss: 3.7485 | LR: 8.87e-05 | GradNorm: 1.854
  Batch    4/48 | Loss: 3.7579 | LR: 9.23e-05 | GradNorm: 5.509
  Batch    8/48 | Loss: 3.7463 | LR: 9.58e-05 | GradNorm: 4.988
  Batch   12/48 | Loss: 3.7147 | LR: 9.93e-05 | GradNorm: 2.756
  Batch   16/48 | Loss: 3.7160 | LR: 1.03e-04 | GradNorm: 4.541
  Batch   20/48 | Loss: 3.7263 | LR: 1.06e-04 | GradNorm: 5.768
  Batch   24/48 | Loss: 3.6525 | LR: 1.10e-04 | GradNorm: 2.124
  Batch   28/48 | Loss: 3.6325 | LR: 1.13e-04 | GradNorm: 4.239
  Batch   32/48 | Loss: 3.6755 | LR: 1.17e-04 | GradNorm: 6.308
  Batch   36/48 | Loss: 3.5884 | LR: 1.21e-04 | GradNorm: 1.976
  Batch   40/48 | Loss: 3.5786 | LR: 1.24e-04 | GradNorm: 5.520
  Batch   44/48 | Loss: 3.5882 | LR: 1.28e-04 | GradNorm: 6.345
Epoch 3/30: Train=7.92%, Val=9.65%, Best=9.65% (E3), Gap=-1.73%, LR=1.30e-04
  Batch    0/48 | Loss: 3.5311 | LR: 1.31e-04 | GradNorm: 5.006
  Batch    4/48 | Loss: 3.5349 | LR: 1.35e-04 | GradNorm: 5.850
  Batch    8/48 | Loss: 3.4904 | LR: 1.38e-04 | GradNorm: 4.854
  Batch   12/48 | Loss: 3.4699 | LR: 1.42e-04 | GradNorm: 8.774
  Batch   16/48 | Loss: 3.4636 | LR: 1.45e-04 | GradNorm: 5.608
  Batch   20/48 | Loss: 3.4354 | LR: 1.49e-04 | GradNorm: 3.710
  Batch   24/48 | Loss: 3.4420 | LR: 1.52e-04 | GradNorm: 5.675
  Batch   28/48 | Loss: 3.4004 | LR: 1.56e-04 | GradNorm: 6.903
  Batch   32/48 | Loss: 3.4711 | LR: 1.59e-04 | GradNorm: 7.605
  Batch   36/48 | Loss: 3.4927 | LR: 1.63e-04 | GradNorm: 7.364
  Batch   40/48 | Loss: 3.4144 | LR: 1.67e-04 | GradNorm: 2.529
  Batch   44/48 | Loss: 3.3886 | LR: 1.70e-04 | GradNorm: 4.492
Epoch 4/30: Train=13.63%, Val=11.47%, Best=11.47% (E4), Gap=2.16%, LR=1.73e-04
  Batch    0/48 | Loss: 3.3382 | LR: 1.74e-04 | GradNorm: 3.481
  Batch    4/48 | Loss: 3.3527 | LR: 1.77e-04 | GradNorm: 7.538
  Batch    8/48 | Loss: 3.2960 | LR: 1.81e-04 | GradNorm: 1.853
  Batch   12/48 | Loss: 3.3530 | LR: 1.84e-04 | GradNorm: 5.441
  Batch   16/48 | Loss: 3.3188 | LR: 1.88e-04 | GradNorm: 1.992
  Batch   20/48 | Loss: 3.3251 | LR: 1.91e-04 | GradNorm: 5.462
  Batch   24/48 | Loss: 3.3594 | LR: 1.95e-04 | GradNorm: 7.817
  Batch   28/48 | Loss: 3.2849 | LR: 1.98e-04 | GradNorm: 4.085
  Batch   32/48 | Loss: 3.2345 | LR: 2.02e-04 | GradNorm: 2.244
  Batch   36/48 | Loss: 3.3601 | LR: 2.05e-04 | GradNorm: 5.684
  Batch   40/48 | Loss: 3.3142 | LR: 2.09e-04 | GradNorm: 6.282
  Batch   44/48 | Loss: 3.2180 | LR: 2.12e-04 | GradNorm: 2.759
Epoch 5/30: Train=16.82%, Val=11.71%, Best=11.71% (E5), Gap=5.11%, LR=2.15e-04
  Batch    0/48 | Loss: 3.2581 | LR: 2.16e-04 | GradNorm: 6.686
  Batch    4/48 | Loss: 3.1538 | LR: 2.20e-04 | GradNorm: 3.410
  Batch    8/48 | Loss: 3.2660 | LR: 2.23e-04 | GradNorm: 7.529
  Batch   12/48 | Loss: 3.1985 | LR: 2.27e-04 | GradNorm: 5.259
  Batch   16/48 | Loss: 3.1788 | LR: 2.30e-04 | GradNorm: 5.978
  Batch   20/48 | Loss: 3.1766 | LR: 2.34e-04 | GradNorm: 3.881
  Batch   24/48 | Loss: 3.1498 | LR: 2.37e-04 | GradNorm: 4.779
  Batch   28/48 | Loss: 3.0466 | LR: 2.41e-04 | GradNorm: 2.753
  Batch   32/48 | Loss: 3.1147 | LR: 2.44e-04 | GradNorm: 6.034
  Batch   36/48 | Loss: 3.1047 | LR: 2.48e-04 | GradNorm: 5.696
  Batch   40/48 | Loss: 3.0863 | LR: 2.51e-04 | GradNorm: 5.105
  Batch   44/48 | Loss: 3.1280 | LR: 2.55e-04 | GradNorm: 5.438
Epoch 6/30: Train=20.64%, Val=14.57%, Best=14.57% (E6), Gap=6.07%, LR=2.58e-04
  Batch    0/48 | Loss: 3.0241 | LR: 2.58e-04 | GradNorm: 4.764
  Batch    4/48 | Loss: 3.0747 | LR: 2.62e-04 | GradNorm: 9.642
  Batch    8/48 | Loss: 3.0145 | LR: 2.66e-04 | GradNorm: 3.228
  Batch   12/48 | Loss: 2.9907 | LR: 2.69e-04 | GradNorm: 6.197
  Batch   16/48 | Loss: 2.9511 | LR: 2.73e-04 | GradNorm: 3.168
  Batch   20/48 | Loss: 3.0505 | LR: 2.76e-04 | GradNorm: 5.369
  Batch   24/48 | Loss: 3.0416 | LR: 2.80e-04 | GradNorm: 5.643
  Batch   28/48 | Loss: 2.9753 | LR: 2.83e-04 | GradNorm: 3.416
  Batch   32/48 | Loss: 3.0465 | LR: 2.87e-04 | GradNorm: 6.520
  Batch   36/48 | Loss: 2.9764 | LR: 2.90e-04 | GradNorm: 5.932
  Batch   40/48 | Loss: 2.9402 | LR: 2.94e-04 | GradNorm: 4.437
  Batch   44/48 | Loss: 2.9281 | LR: 2.97e-04 | GradNorm: 3.613
/localdrive/users/dkreinov/quickdraw-mobilevit-quant/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:209: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Epoch 7/30: Train=25.05%, Val=16.37%, Best=16.37% (E7), Gap=8.68%, LR=3.00e-04
  Batch    0/48 | Loss: 2.9448 | LR: 3.00e-04 | GradNorm: 5.470
  Batch    4/48 | Loss: 2.9606 | LR: 3.00e-04 | GradNorm: 10.879
  Batch    8/48 | Loss: 2.9238 | LR: 3.00e-04 | GradNorm: 5.899
  Batch   12/48 | Loss: 2.8797 | LR: 3.00e-04 | GradNorm: 4.167
  Batch   16/48 | Loss: 2.9102 | LR: 3.00e-04 | GradNorm: 5.431
  Batch   20/48 | Loss: 2.7865 | LR: 3.00e-04 | GradNorm: 4.986
  Batch   24/48 | Loss: 2.7627 | LR: 3.00e-04 | GradNorm: 3.357
  Batch   28/48 | Loss: 2.8777 | LR: 2.99e-04 | GradNorm: 4.862
  Batch   32/48 | Loss: 2.8680 | LR: 2.99e-04 | GradNorm: 7.659
  Batch   36/48 | Loss: 2.7871 | LR: 2.99e-04 | GradNorm: 4.962
  Batch   40/48 | Loss: 2.7431 | LR: 2.99e-04 | GradNorm: 5.749
  Batch   44/48 | Loss: 2.7811 | LR: 2.99e-04 | GradNorm: 5.119
Epoch 8/30: Train=29.58%, Val=20.07%, Best=20.07% (E8), Gap=9.51%, LR=2.99e-04
  Batch    0/48 | Loss: 2.7007 | LR: 2.99e-04 | GradNorm: 3.806
  Batch    4/48 | Loss: 2.7001 | LR: 2.98e-04 | GradNorm: 5.332
  Batch    8/48 | Loss: 2.7294 | LR: 2.98e-04 | GradNorm: 5.075
  Batch   12/48 | Loss: 2.6768 | LR: 2.98e-04 | GradNorm: 6.082
  Batch   16/48 | Loss: 2.6365 | LR: 2.97e-04 | GradNorm: 5.439
  Batch   20/48 | Loss: 2.6140 | LR: 2.97e-04 | GradNorm: 4.800
  Batch   24/48 | Loss: 2.6247 | LR: 2.97e-04 | GradNorm: 7.768
  Batch   28/48 | Loss: 2.6646 | LR: 2.96e-04 | GradNorm: 7.416
  Batch   32/48 | Loss: 2.5984 | LR: 2.96e-04 | GradNorm: 5.815
  Batch   36/48 | Loss: 2.6866 | LR: 2.96e-04 | GradNorm: 13.209
  Batch   40/48 | Loss: 2.6175 | LR: 2.95e-04 | GradNorm: 6.502
  Batch   44/48 | Loss: 2.6075 | LR: 2.95e-04 | GradNorm: 4.330
Epoch 9/30: Train=35.81%, Val=21.07%, Best=21.07% (E9), Gap=14.74%, LR=2.94e-04
  Batch    0/48 | Loss: 2.5435 | LR: 2.94e-04 | GradNorm: 4.832
  Batch    4/48 | Loss: 2.5904 | LR: 2.94e-04 | GradNorm: 8.591
  Batch    8/48 | Loss: 2.5661 | LR: 2.93e-04 | GradNorm: 13.171
  Batch   12/48 | Loss: 2.6889 | LR: 2.93e-04 | GradNorm: 17.793
  Batch   16/48 | Loss: 2.6631 | LR: 2.92e-04 | GradNorm: 9.158
  Batch   20/48 | Loss: 2.5254 | LR: 2.92e-04 | GradNorm: 3.806
  Batch   24/48 | Loss: 2.5611 | LR: 2.91e-04 | GradNorm: 6.172
  Batch   28/48 | Loss: 2.5288 | LR: 2.91e-04 | GradNorm: 3.620
  Batch   32/48 | Loss: 2.5548 | LR: 2.90e-04 | GradNorm: 5.464
  Batch   36/48 | Loss: 2.4850 | LR: 2.89e-04 | GradNorm: 4.261
  Batch   40/48 | Loss: 2.5074 | LR: 2.89e-04 | GradNorm: 9.874
  Batch   44/48 | Loss: 2.4610 | LR: 2.88e-04 | GradNorm: 6.300
Epoch 10/30: Train=38.66%, Val=22.70%, Best=22.70% (E10), Gap=15.96%, LR=2.88e-04
  Batch    0/48 | Loss: 2.4409 | LR: 2.87e-04 | GradNorm: 6.947
  Batch    4/48 | Loss: 2.5170 | LR: 2.87e-04 | GradNorm: 7.567
  Batch    8/48 | Loss: 2.4362 | LR: 2.86e-04 | GradNorm: 4.157
  Batch   12/48 | Loss: 2.5143 | LR: 2.85e-04 | GradNorm: 7.830
  Batch   16/48 | Loss: 2.4165 | LR: 2.85e-04 | GradNorm: 5.814
  Batch   20/48 | Loss: 2.4745 | LR: 2.84e-04 | GradNorm: 7.913
  Batch   24/48 | Loss: 2.5477 | LR: 2.83e-04 | GradNorm: 7.214
  Batch   28/48 | Loss: 2.3469 | LR: 2.82e-04 | GradNorm: 10.170
  Batch   32/48 | Loss: 2.4894 | LR: 2.81e-04 | GradNorm: 12.761
  Batch   36/48 | Loss: 2.4800 | LR: 2.81e-04 | GradNorm: 11.982
  Batch   40/48 | Loss: 2.4366 | LR: 2.80e-04 | GradNorm: 8.152
  Batch   44/48 | Loss: 2.4011 | LR: 2.79e-04 | GradNorm: 7.498
Epoch 11/30: Train=42.38%, Val=26.60%, Best=26.60% (E11), Gap=15.78%, LR=2.78e-04
  Batch    0/48 | Loss: 2.3634 | LR: 2.78e-04 | GradNorm: 5.757
  Batch    4/48 | Loss: 2.4102 | LR: 2.77e-04 | GradNorm: 4.110
  Batch    8/48 | Loss: 2.4033 | LR: 2.76e-04 | GradNorm: 7.449
  Batch   12/48 | Loss: 2.4272 | LR: 2.75e-04 | GradNorm: 7.174
  Batch   16/48 | Loss: 2.3704 | LR: 2.74e-04 | GradNorm: 4.759
  Batch   20/48 | Loss: 2.4138 | LR: 2.73e-04 | GradNorm: 5.300
  Batch   24/48 | Loss: 2.4184 | LR: 2.72e-04 | GradNorm: 7.973
  Batch   28/48 | Loss: 2.3926 | LR: 2.71e-04 | GradNorm: 6.955
  Batch   32/48 | Loss: 2.3534 | LR: 2.70e-04 | GradNorm: 5.398
  Batch   36/48 | Loss: 2.2773 | LR: 2.69e-04 | GradNorm: 5.901
  Batch   40/48 | Loss: 2.2623 | LR: 2.68e-04 | GradNorm: 4.602
  Batch   44/48 | Loss: 2.2793 | LR: 2.67e-04 | GradNorm: 5.010
Epoch 12/30: Train=45.26%, Val=27.26%, Best=27.26% (E12), Gap=18.00%, LR=2.66e-04
  Batch    0/48 | Loss: 2.3467 | LR: 2.66e-04 | GradNorm: 5.301
  Batch    4/48 | Loss: 2.3146 | LR: 2.65e-04 | GradNorm: 7.149
  Batch    8/48 | Loss: 2.2252 | LR: 2.64e-04 | GradNorm: 7.668
  Batch   12/48 | Loss: 2.2070 | LR: 2.63e-04 | GradNorm: 5.201
  Batch   16/48 | Loss: 2.2113 | LR: 2.62e-04 | GradNorm: 6.273
  Batch   20/48 | Loss: 2.2710 | LR: 2.61e-04 | GradNorm: 7.601
  Batch   24/48 | Loss: 2.2857 | LR: 2.59e-04 | GradNorm: 5.943
  Batch   28/48 | Loss: 2.2515 | LR: 2.58e-04 | GradNorm: 5.891
  Batch   32/48 | Loss: 2.2230 | LR: 2.57e-04 | GradNorm: 5.083
  Batch   36/48 | Loss: 2.2398 | LR: 2.56e-04 | GradNorm: 5.742
  Batch   40/48 | Loss: 2.2583 | LR: 2.55e-04 | GradNorm: 7.353
  Batch   44/48 | Loss: 2.2085 | LR: 2.53e-04 | GradNorm: 4.576
Epoch 13/30: Train=48.78%, Val=36.90%, Best=36.90% (E13), Gap=11.88%, LR=2.52e-04
  Batch    0/48 | Loss: 2.2462 | LR: 2.52e-04 | GradNorm: 5.630
  Batch    4/48 | Loss: 2.2386 | LR: 2.51e-04 | GradNorm: 6.106
  Batch    8/48 | Loss: 2.2906 | LR: 2.50e-04 | GradNorm: 7.499
  Batch   12/48 | Loss: 2.1651 | LR: 2.48e-04 | GradNorm: 4.791
  Batch   16/48 | Loss: 2.2312 | LR: 2.47e-04 | GradNorm: 6.768
  Batch   20/48 | Loss: 2.1098 | LR: 2.46e-04 | GradNorm: 4.628
  Batch   24/48 | Loss: 2.1803 | LR: 2.44e-04 | GradNorm: 5.295
  Batch   28/48 | Loss: 2.1903 | LR: 2.43e-04 | GradNorm: 6.493
  Batch   32/48 | Loss: 2.1185 | LR: 2.42e-04 | GradNorm: 5.062
  Batch   36/48 | Loss: 2.1331 | LR: 2.40e-04 | GradNorm: 6.097
  Batch   40/48 | Loss: 2.1427 | LR: 2.39e-04 | GradNorm: 5.787
  Batch   44/48 | Loss: 2.1057 | LR: 2.38e-04 | GradNorm: 5.614
Epoch 14/30: Train=51.62%, Val=36.57%, Best=36.90% (E13), Gap=15.05%, LR=2.37e-04
  Batch    0/48 | Loss: 2.1395 | LR: 2.36e-04 | GradNorm: 6.878
  Batch    4/48 | Loss: 2.1457 | LR: 2.35e-04 | GradNorm: 8.649
  Batch    8/48 | Loss: 2.1139 | LR: 2.33e-04 | GradNorm: 6.926
  Batch   12/48 | Loss: 2.1182 | LR: 2.32e-04 | GradNorm: 3.854
  Batch   16/48 | Loss: 2.1073 | LR: 2.30e-04 | GradNorm: 5.218
  Batch   20/48 | Loss: 2.0481 | LR: 2.29e-04 | GradNorm: 3.844
  Batch   24/48 | Loss: 2.0809 | LR: 2.28e-04 | GradNorm: 6.846
  Batch   28/48 | Loss: 2.0554 | LR: 2.26e-04 | GradNorm: 5.545
  Batch   32/48 | Loss: 2.1183 | LR: 2.25e-04 | GradNorm: 5.181
  Batch   36/48 | Loss: 2.0681 | LR: 2.23e-04 | GradNorm: 3.911
  Batch   40/48 | Loss: 2.0353 | LR: 2.22e-04 | GradNorm: 6.407
  Batch   44/48 | Loss: 2.0428 | LR: 2.20e-04 | GradNorm: 4.562
⚠️  Very low performance: 34.48% < 75.0% (Count: 1/5)
Epoch 15/30: Train=54.39%, Val=34.48%, Best=36.90% (E13), Gap=19.91%, LR=2.19e-04
  Batch    0/48 | Loss: 2.1541 | LR: 2.19e-04 | GradNorm: 7.316
  Batch    4/48 | Loss: 2.0269 | LR: 2.17e-04 | GradNorm: 4.457
  Batch    8/48 | Loss: 2.0293 | LR: 2.16e-04 | GradNorm: 5.065
  Batch   12/48 | Loss: 2.0269 | LR: 2.14e-04 | GradNorm: 4.526
  Batch   16/48 | Loss: 2.0280 | LR: 2.13e-04 | GradNorm: 5.395
  Batch   20/48 | Loss: 2.0521 | LR: 2.11e-04 | GradNorm: 6.674
  Batch   24/48 | Loss: 2.0438 | LR: 2.09e-04 | GradNorm: 5.512
  Batch   28/48 | Loss: 2.0050 | LR: 2.08e-04 | GradNorm: 4.917
  Batch   32/48 | Loss: 1.9324 | LR: 2.06e-04 | GradNorm: 4.262
  Batch   36/48 | Loss: 2.0292 | LR: 2.05e-04 | GradNorm: 4.263
  Batch   40/48 | Loss: 2.0306 | LR: 2.03e-04 | GradNorm: 5.050
  Batch   44/48 | Loss: 2.0343 | LR: 2.01e-04 | GradNorm: 4.902
⚠️  Very low performance: 40.18% < 75.0% (Count: 2/5)
Epoch 16/30: Train=56.56%, Val=40.18%, Best=40.18% (E16), Gap=16.38%, LR=2.00e-04
  Batch    0/48 | Loss: 2.0147 | LR: 2.00e-04 | GradNorm: 5.704
  Batch    4/48 | Loss: 1.9787 | LR: 1.98e-04 | GradNorm: 3.974
  Batch    8/48 | Loss: 1.9443 | LR: 1.97e-04 | GradNorm: 4.456
  Batch   12/48 | Loss: 1.9751 | LR: 1.95e-04 | GradNorm: 5.876
  Batch   16/48 | Loss: 1.9810 | LR: 1.93e-04 | GradNorm: 6.296
  Batch   20/48 | Loss: 1.9203 | LR: 1.92e-04 | GradNorm: 4.096
  Batch   24/48 | Loss: 1.9984 | LR: 1.90e-04 | GradNorm: 5.443
  Batch   28/48 | Loss: 1.9264 | LR: 1.88e-04 | GradNorm: 4.508
  Batch   32/48 | Loss: 1.9018 | LR: 1.87e-04 | GradNorm: 3.090
  Batch   36/48 | Loss: 1.9839 | LR: 1.85e-04 | GradNorm: 4.816
  Batch   40/48 | Loss: 1.9696 | LR: 1.83e-04 | GradNorm: 6.021
  Batch   44/48 | Loss: 1.9856 | LR: 1.82e-04 | GradNorm: 5.240
⚠️  Very low performance: 39.43% < 75.0% (Count: 3/5)
Epoch 17/30: Train=58.63%, Val=39.43%, Best=40.18% (E16), Gap=19.20%, LR=1.81e-04
  Batch    0/48 | Loss: 1.9371 | LR: 1.80e-04 | GradNorm: 6.104
  Batch    4/48 | Loss: 1.9054 | LR: 1.78e-04 | GradNorm: 5.878
  Batch    8/48 | Loss: 1.8950 | LR: 1.77e-04 | GradNorm: 4.271
  Batch   12/48 | Loss: 1.9585 | LR: 1.75e-04 | GradNorm: 4.574
  Batch   16/48 | Loss: 1.8686 | LR: 1.73e-04 | GradNorm: 6.578
  Batch   20/48 | Loss: 1.9337 | LR: 1.72e-04 | GradNorm: 4.415
  Batch   24/48 | Loss: 1.9116 | LR: 1.70e-04 | GradNorm: 4.239
  Batch   28/48 | Loss: 1.9415 | LR: 1.68e-04 | GradNorm: 5.243
  Batch   32/48 | Loss: 1.9198 | LR: 1.67e-04 | GradNorm: 3.839
  Batch   36/48 | Loss: 2.0186 | LR: 1.65e-04 | GradNorm: 6.455
  Batch   40/48 | Loss: 1.9278 | LR: 1.63e-04 | GradNorm: 6.377
  Batch   44/48 | Loss: 1.8579 | LR: 1.62e-04 | GradNorm: 5.150
⚠️  Very low performance: 45.84% < 75.0% (Count: 4/5)
Epoch 18/30: Train=60.12%, Val=45.84%, Best=45.84% (E18), Gap=14.28%, LR=1.60e-04
  Batch    0/48 | Loss: 1.7939 | LR: 1.60e-04 | GradNorm: 4.855
  Batch    4/48 | Loss: 1.9597 | LR: 1.58e-04 | GradNorm: 5.308
  Batch    8/48 | Loss: 1.9046 | LR: 1.56e-04 | GradNorm: 5.928
  Batch   12/48 | Loss: 1.9067 | LR: 1.55e-04 | GradNorm: 5.643
  Batch   16/48 | Loss: 1.9148 | LR: 1.53e-04 | GradNorm: 6.714
  Batch   20/48 | Loss: 1.8332 | LR: 1.51e-04 | GradNorm: 5.328
  Batch   24/48 | Loss: 1.8828 | LR: 1.50e-04 | GradNorm: 6.691
  Batch   28/48 | Loss: 1.8585 | LR: 1.48e-04 | GradNorm: 6.732
  Batch   32/48 | Loss: 1.8250 | LR: 1.46e-04 | GradNorm: 4.741
  Batch   36/48 | Loss: 1.8357 | LR: 1.45e-04 | GradNorm: 4.284
  Batch   40/48 | Loss: 1.8472 | LR: 1.43e-04 | GradNorm: 6.018
  Batch   44/48 | Loss: 1.8371 | LR: 1.41e-04 | GradNorm: 4.329
⚠️  Very low performance: 47.49% < 75.0% (Count: 5/5)
🛑 Early stopping: 5 consecutive epochs below 75.0%
   Current performance: 47.49%
Final best validation accuracy: 47.49% (epoch 19)
