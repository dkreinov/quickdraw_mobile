=== LAMB Sweep Run ===
LR: 0.000552
Weight Decay: 0.1147
Warmup Epochs: 5
Using device: cuda
Available GPUs: 4
‚è±Ô∏è  Creating train dataset...
Loading QuickDraw dataset from: ../data/quickdraw_parquet
Available classes: 344
Using 50 classes: ['aircraft carrier', 'arm', 'asparagus', 'backpack', 'banana', 'basketball', 'bottlecap', 'bread', 'broom', 'bulldozer']...
Using per-class Parquet format (recommended)...
  Loading 50 classes concurrently from per-class files...
    aircraft carrier: 1200 samples
    arm: 1200 samples
    asparagus: 1200 samples
    backpack: 1200 samples
    banana: 1200 samples
    basketball: 1200 samples
    bottlecap: 1200 samples
    bread: 1200 samples
    broom: 1200 samples
    bulldozer: 1200 samples
    butterfly: 1200 samples
    camel: 1200 samples
    canoe: 1200 samples
    chair: 1200 samples
    compass: 1200 samples
    cookie: 1200 samples
    drums: 1200 samples
    eyeglasses: 1200 samples
    face: 1200 samples
    fan: 1200 samples
    fence: 1200 samples
    fish: 1200 samples
    flying saucer: 1200 samples
    grapes: 1200 samples
    hand: 1200 samples
    hat: 1200 samples
    horse: 1200 samples
    light bulb: 1200 samples
    lighthouse: 1200 samples
    line: 1200 samples
    marker: 1200 samples
    mountain: 1200 samples
    mouse: 1200 samples
    parachute: 1200 samples
    passport: 1200 samples
    pliers: 1200 samples
    potato: 1200 samples
    sea turtle: 1200 samples
    snowflake: 1200 samples
    spider: 1200 samples
    square: 1200 samples
    steak: 1200 samples
    swing set: 1200 samples
    sword: 1200 samples
    television: 1200 samples
    tennis racquet: 1200 samples
    toothbrush: 1200 samples
    train: 1200 samples
    umbrella: 1200 samples
    washing machine: 1200 samples
  Per-class loading complete (concurrent): 60000 samples
Total samples: 60000
‚è±Ô∏è  Train dataset creation took: 0.47s
‚è±Ô∏è  Creating val dataset...
Loading QuickDraw dataset from: ../data/quickdraw_parquet
Available classes: 344
Using 50 classes: ['aircraft carrier', 'arm', 'asparagus', 'backpack', 'banana', 'basketball', 'bottlecap', 'bread', 'broom', 'bulldozer']...
Using per-class Parquet format (recommended)...
  Loading 50 classes concurrently from per-class files...
    aircraft carrier: 1200 samples
    arm: 1200 samples
    asparagus: 1200 samples
    backpack: 1200 samples
    banana: 1200 samples
    basketball: 1200 samples
    bottlecap: 1200 samples
    bread: 1200 samples
    broom: 1200 samples
    bulldozer: 1200 samples
    butterfly: 1200 samples
    camel: 1200 samples
    canoe: 1200 samples
    chair: 1200 samples
    compass: 1200 samples
    cookie: 1200 samples
    drums: 1200 samples
    eyeglasses: 1200 samples
    face: 1200 samples
    fan: 1200 samples
    fence: 1200 samples
    fish: 1200 samples
    flying saucer: 1200 samples
    grapes: 1200 samples
    hand: 1200 samples
    hat: 1200 samples
    horse: 1200 samples
    light bulb: 1200 samples
    lighthouse: 1200 samples
    line: 1200 samples
    marker: 1200 samples
    mountain: 1200 samples
    mouse: 1200 samples
    parachute: 1200 samples
    passport: 1200 samples
    pliers: 1200 samples
    potato: 1200 samples
    sea turtle: 1200 samples
    snowflake: 1200 samples
    spider: 1200 samples
    square: 1200 samples
    steak: 1200 samples
    swing set: 1200 samples
    sword: 1200 samples
    television: 1200 samples
    tennis racquet: 1200 samples
    toothbrush: 1200 samples
    train: 1200 samples
    umbrella: 1200 samples
    washing machine: 1200 samples
  Per-class loading complete (concurrent): 60000 samples
Total samples: 60000
‚è±Ô∏è  Val dataset creation took: 0.45s

Loading train/val split...
Using pre-computed split (fast loading)
‚è±Ô∏è  Split loading took: 0.01s
‚è±Ô∏è  Subset creation took: 0.00s
‚è±Ô∏è  DataLoader creation took: 0.00s
‚è±Ô∏è  TOTAL data loading time: 0.02s

Dataloaders created:
  Training: 48 batches (50000 samples)
  Validation: 10 batches (10000 samples)
Dataset: 50 classes, 48 train batches

Building model:
   Architecture: vit_tiny_patch16_224
   Input channels: 1 (grayscale)
   Output classes: 50
Creating vit_tiny_patch16_224 with 50 classes
   Single-channel input: 1 ‚Üí 3 channel conversion
   Pretrained: False
   Drop path rate: 0.1
   Random initialization for single-channel weights

Model info:
   Total parameters: 5,435,762
   Trainable parameters: 5,435,762
   Model size: 20.74 MB
Using 4 GPUs with LAMB optimizer
Setting up deterministic training (seed=42)
Created lamb (fallback to AdamW) optimizer:
   Learning rate: 0.000552
   Weight decay: 0.11468312530902586
   Decay params: 52
   No-decay params: 100
Created learning rate scheduler (step-based):
   Warmup steps: 240
   Total steps: 1440
   Steps per epoch: 48
Created loss function:
   Type: CrossEntropyLoss
   Label smoothing: 0.1
Created AMP gradient scaler
Trainer initialized:
  Model: lamb_lr_0.000552_wd_0.1147
  Train batches: 48
  Val batches: 10
  Save directory: experiments/wandb_lamb_runs
Starting LAMB training for 30 epochs...
/localdrive/users/dkreinov/quickdraw-mobilevit-quant/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:233.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/localdrive/users/dkreinov/quickdraw-mobilevit-quant/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: Memory Efficient attention defaults to a non-deterministic algorithm. To explicitly enable determinism call torch.use_deterministic_algorithms(True, warn_only=False). (Triggered internally at /pytorch/aten/src/ATen/native/transformers/cuda/attention_backward.cu:775.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  Batch    0/48 | Loss: 3.9436 | LR: 7.79e-06 | GradNorm: 1.449
  Batch    4/48 | Loss: 3.9215 | LR: 1.69e-05 | GradNorm: 0.947
  Batch    8/48 | Loss: 3.9192 | LR: 2.60e-05 | GradNorm: 0.894
  Batch   12/48 | Loss: 3.9108 | LR: 3.51e-05 | GradNorm: 0.677
  Batch   16/48 | Loss: 3.9209 | LR: 4.42e-05 | GradNorm: 0.763
  Batch   20/48 | Loss: 3.9147 | LR: 5.33e-05 | GradNorm: 0.715
  Batch   24/48 | Loss: 3.9152 | LR: 6.24e-05 | GradNorm: 0.714
  Batch   28/48 | Loss: 3.9195 | LR: 7.15e-05 | GradNorm: 0.688
  Batch   32/48 | Loss: 3.9097 | LR: 8.06e-05 | GradNorm: 0.569
  Batch   36/48 | Loss: 3.9078 | LR: 8.97e-05 | GradNorm: 0.546
  Batch   40/48 | Loss: 3.8986 | LR: 9.88e-05 | GradNorm: 0.594
  Batch   44/48 | Loss: 3.8912 | LR: 1.08e-04 | GradNorm: 0.693
Epoch 1/30: Train=2.50%, Val=5.62%, Best=5.62% (E1), Gap=-3.12%, LR=1.15e-04
  Batch    0/48 | Loss: 3.8589 | LR: 1.17e-04 | GradNorm: 0.906
  Batch    4/48 | Loss: 3.9025 | LR: 1.26e-04 | GradNorm: 7.455
  Batch    8/48 | Loss: 3.8856 | LR: 1.35e-04 | GradNorm: 7.851
  Batch   12/48 | Loss: 3.8582 | LR: 1.44e-04 | GradNorm: 8.759
  Batch   16/48 | Loss: 3.8483 | LR: 1.53e-04 | GradNorm: 7.233
  Batch   20/48 | Loss: 3.7392 | LR: 1.63e-04 | GradNorm: 0.982
  Batch   24/48 | Loss: 3.7875 | LR: 1.72e-04 | GradNorm: 8.928
  Batch   28/48 | Loss: 3.7947 | LR: 1.81e-04 | GradNorm: 5.197
  Batch   32/48 | Loss: 3.7281 | LR: 1.90e-04 | GradNorm: 3.525
  Batch   36/48 | Loss: 3.7096 | LR: 1.99e-04 | GradNorm: 2.769
  Batch   40/48 | Loss: 3.6820 | LR: 2.08e-04 | GradNorm: 1.973
  Batch   44/48 | Loss: 3.6546 | LR: 2.17e-04 | GradNorm: 4.811
Epoch 2/30: Train=5.67%, Val=6.69%, Best=6.69% (E2), Gap=-1.02%, LR=2.24e-04
  Batch    0/48 | Loss: 3.6446 | LR: 2.26e-04 | GradNorm: 5.428
  Batch    4/48 | Loss: 3.6360 | LR: 2.35e-04 | GradNorm: 1.715
  Batch    8/48 | Loss: 3.5624 | LR: 2.44e-04 | GradNorm: 1.827
  Batch   12/48 | Loss: 3.5608 | LR: 2.54e-04 | GradNorm: 1.489
  Batch   16/48 | Loss: 3.6217 | LR: 2.63e-04 | GradNorm: 9.643
  Batch   20/48 | Loss: 3.5022 | LR: 2.72e-04 | GradNorm: 2.482
  Batch   24/48 | Loss: 3.5094 | LR: 2.81e-04 | GradNorm: 3.685
  Batch   28/48 | Loss: 3.4163 | LR: 2.90e-04 | GradNorm: 1.154
  Batch   32/48 | Loss: 3.4805 | LR: 2.99e-04 | GradNorm: 5.677
  Batch   36/48 | Loss: 3.4113 | LR: 3.08e-04 | GradNorm: 3.156
  Batch   40/48 | Loss: 3.3779 | LR: 3.17e-04 | GradNorm: 4.186
  Batch   44/48 | Loss: 3.3775 | LR: 3.26e-04 | GradNorm: 4.539
Epoch 3/30: Train=12.22%, Val=13.79%, Best=13.79% (E3), Gap=-1.57%, LR=3.33e-04
  Batch    0/48 | Loss: 3.3314 | LR: 3.35e-04 | GradNorm: 3.828
  Batch    4/48 | Loss: 3.3784 | LR: 3.45e-04 | GradNorm: 3.653
  Batch    8/48 | Loss: 3.2633 | LR: 3.54e-04 | GradNorm: 3.078
  Batch   12/48 | Loss: 3.2139 | LR: 3.63e-04 | GradNorm: 2.183
  Batch   16/48 | Loss: 3.4919 | LR: 3.72e-04 | GradNorm: 8.977
  Batch   20/48 | Loss: 3.3815 | LR: 3.81e-04 | GradNorm: 5.653
  Batch   24/48 | Loss: 3.3259 | LR: 3.90e-04 | GradNorm: 4.532
  Batch   28/48 | Loss: 3.2365 | LR: 3.99e-04 | GradNorm: 2.960
  Batch   32/48 | Loss: 3.2261 | LR: 4.08e-04 | GradNorm: 3.112
  Batch   36/48 | Loss: 3.1729 | LR: 4.17e-04 | GradNorm: 2.813
  Batch   40/48 | Loss: 3.2076 | LR: 4.26e-04 | GradNorm: 3.909
  Batch   44/48 | Loss: 3.1362 | LR: 4.36e-04 | GradNorm: 3.256
Epoch 4/30: Train=17.35%, Val=11.76%, Best=13.79% (E3), Gap=5.59%, LR=4.42e-04
  Batch    0/48 | Loss: 3.1007 | LR: 4.45e-04 | GradNorm: 5.348
  Batch    4/48 | Loss: 3.0751 | LR: 4.54e-04 | GradNorm: 5.461
  Batch    8/48 | Loss: 3.1172 | LR: 4.63e-04 | GradNorm: 7.594
  Batch   12/48 | Loss: 3.0841 | LR: 4.72e-04 | GradNorm: 3.525
  Batch   16/48 | Loss: 3.1675 | LR: 4.81e-04 | GradNorm: 6.501
  Batch   20/48 | Loss: 3.1707 | LR: 4.90e-04 | GradNorm: 4.319
  Batch   24/48 | Loss: 3.1740 | LR: 4.99e-04 | GradNorm: 5.605
  Batch   28/48 | Loss: 3.1918 | LR: 5.08e-04 | GradNorm: 5.039
  Batch   32/48 | Loss: 3.1067 | LR: 5.17e-04 | GradNorm: 4.905
  Batch   36/48 | Loss: 3.1214 | LR: 5.27e-04 | GradNorm: 4.050
  Batch   40/48 | Loss: 3.1196 | LR: 5.36e-04 | GradNorm: 4.049
  Batch   44/48 | Loss: 3.0541 | LR: 5.45e-04 | GradNorm: 4.294
/localdrive/users/dkreinov/quickdraw-mobilevit-quant/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:209: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Epoch 5/30: Train=20.88%, Val=14.38%, Best=14.38% (E5), Gap=6.50%, LR=5.52e-04
  Batch    0/48 | Loss: 3.0088 | LR: 5.52e-04 | GradNorm: 6.437
  Batch    4/48 | Loss: 2.9531 | LR: 5.52e-04 | GradNorm: 4.913
  Batch    8/48 | Loss: 3.0850 | LR: 5.52e-04 | GradNorm: 7.848
  Batch   12/48 | Loss: 2.9806 | LR: 5.51e-04 | GradNorm: 3.795
  Batch   16/48 | Loss: 2.9433 | LR: 5.51e-04 | GradNorm: 3.162
  Batch   20/48 | Loss: 2.9299 | LR: 5.51e-04 | GradNorm: 3.244
  Batch   24/48 | Loss: 2.9860 | LR: 5.51e-04 | GradNorm: 4.483
  Batch   28/48 | Loss: 2.9481 | LR: 5.51e-04 | GradNorm: 5.805
  Batch   32/48 | Loss: 2.8969 | LR: 5.51e-04 | GradNorm: 5.830
  Batch   36/48 | Loss: 2.8868 | LR: 5.50e-04 | GradNorm: 4.485
  Batch   40/48 | Loss: 2.8590 | LR: 5.50e-04 | GradNorm: 2.866
  Batch   44/48 | Loss: 2.8822 | LR: 5.50e-04 | GradNorm: 6.221
Epoch 6/30: Train=25.73%, Val=19.39%, Best=19.39% (E6), Gap=6.34%, LR=5.49e-04
  Batch    0/48 | Loss: 2.8824 | LR: 5.49e-04 | GradNorm: 7.545
  Batch    4/48 | Loss: 2.8200 | LR: 5.49e-04 | GradNorm: 3.628
  Batch    8/48 | Loss: 2.8533 | LR: 5.49e-04 | GradNorm: 5.230
  Batch   12/48 | Loss: 2.7023 | LR: 5.48e-04 | GradNorm: 2.441
  Batch   16/48 | Loss: 2.7676 | LR: 5.48e-04 | GradNorm: 3.540
  Batch   20/48 | Loss: 2.7643 | LR: 5.47e-04 | GradNorm: 3.163
  Batch   24/48 | Loss: 2.8011 | LR: 5.47e-04 | GradNorm: 5.784
  Batch   28/48 | Loss: 2.8063 | LR: 5.46e-04 | GradNorm: 4.483
  Batch   32/48 | Loss: 2.8609 | LR: 5.45e-04 | GradNorm: 6.713
  Batch   36/48 | Loss: 2.7932 | LR: 5.45e-04 | GradNorm: 4.303
  Batch   40/48 | Loss: 2.7701 | LR: 5.44e-04 | GradNorm: 3.690
  Batch   44/48 | Loss: 2.6732 | LR: 5.43e-04 | GradNorm: 2.796
Epoch 7/30: Train=30.21%, Val=18.96%, Best=19.39% (E6), Gap=11.25%, LR=5.43e-04
  Batch    0/48 | Loss: 2.6906 | LR: 5.43e-04 | GradNorm: 4.935
  Batch    4/48 | Loss: 2.7346 | LR: 5.42e-04 | GradNorm: 6.067
  Batch    8/48 | Loss: 2.7079 | LR: 5.41e-04 | GradNorm: 4.565
  Batch   12/48 | Loss: 2.6910 | LR: 5.40e-04 | GradNorm: 2.428
  Batch   16/48 | Loss: 2.7180 | LR: 5.40e-04 | GradNorm: 4.529
  Batch   20/48 | Loss: 2.5857 | LR: 5.39e-04 | GradNorm: 4.254
  Batch   24/48 | Loss: 2.6375 | LR: 5.38e-04 | GradNorm: 6.400
  Batch   28/48 | Loss: 2.6837 | LR: 5.37e-04 | GradNorm: 7.973
  Batch   32/48 | Loss: 2.6296 | LR: 5.36e-04 | GradNorm: 5.965
  Batch   36/48 | Loss: 2.5727 | LR: 5.35e-04 | GradNorm: 4.692
  Batch   40/48 | Loss: 2.4584 | LR: 5.34e-04 | GradNorm: 5.373
  Batch   44/48 | Loss: 2.5265 | LR: 5.33e-04 | GradNorm: 3.072
Epoch 8/30: Train=35.71%, Val=27.24%, Best=27.24% (E8), Gap=8.47%, LR=5.32e-04
  Batch    0/48 | Loss: 2.4921 | LR: 5.32e-04 | GradNorm: 5.602
  Batch    4/48 | Loss: 2.4768 | LR: 5.31e-04 | GradNorm: 6.001
  Batch    8/48 | Loss: 2.4871 | LR: 5.30e-04 | GradNorm: 3.803
  Batch   12/48 | Loss: 2.4741 | LR: 5.29e-04 | GradNorm: 5.843
  Batch   16/48 | Loss: 2.4245 | LR: 5.27e-04 | GradNorm: 4.372
  Batch   20/48 | Loss: 2.3942 | LR: 5.26e-04 | GradNorm: 5.027
  Batch   24/48 | Loss: 2.4995 | LR: 5.25e-04 | GradNorm: 9.275
  Batch   28/48 | Loss: 2.4728 | LR: 5.24e-04 | GradNorm: 7.304
  Batch   32/48 | Loss: 2.4086 | LR: 5.23e-04 | GradNorm: 4.619
  Batch   36/48 | Loss: 2.5459 | LR: 5.21e-04 | GradNorm: 10.384
  Batch   40/48 | Loss: 2.3788 | LR: 5.20e-04 | GradNorm: 4.768
  Batch   44/48 | Loss: 2.4374 | LR: 5.19e-04 | GradNorm: 6.913
Epoch 9/30: Train=41.20%, Val=23.73%, Best=27.24% (E8), Gap=17.47%, LR=5.17e-04
  Batch    0/48 | Loss: 2.4045 | LR: 5.17e-04 | GradNorm: 7.112
  Batch    4/48 | Loss: 2.3753 | LR: 5.16e-04 | GradNorm: 4.405
  Batch    8/48 | Loss: 2.3278 | LR: 5.14e-04 | GradNorm: 4.244
  Batch   12/48 | Loss: 2.4011 | LR: 5.13e-04 | GradNorm: 5.422
  Batch   16/48 | Loss: 2.4022 | LR: 5.11e-04 | GradNorm: 7.331
  Batch   20/48 | Loss: 2.4627 | LR: 5.10e-04 | GradNorm: 6.841
  Batch   24/48 | Loss: 2.3894 | LR: 5.08e-04 | GradNorm: 3.763
  Batch   28/48 | Loss: 2.4138 | LR: 5.07e-04 | GradNorm: 5.717
  Batch   32/48 | Loss: 2.4813 | LR: 5.05e-04 | GradNorm: 7.702
  Batch   36/48 | Loss: 2.3793 | LR: 5.03e-04 | GradNorm: 7.580
  Batch   40/48 | Loss: 2.3258 | LR: 5.02e-04 | GradNorm: 4.535
  Batch   44/48 | Loss: 2.2980 | LR: 5.00e-04 | GradNorm: 4.917
Epoch 10/30: Train=43.07%, Val=28.54%, Best=28.54% (E10), Gap=14.53%, LR=4.99e-04
  Batch    0/48 | Loss: 2.2585 | LR: 4.98e-04 | GradNorm: 3.565
  Batch    4/48 | Loss: 2.2828 | LR: 4.97e-04 | GradNorm: 5.298
  Batch    8/48 | Loss: 2.2449 | LR: 4.95e-04 | GradNorm: 3.763
  Batch   12/48 | Loss: 2.3398 | LR: 4.93e-04 | GradNorm: 7.061
  Batch   16/48 | Loss: 2.2028 | LR: 4.91e-04 | GradNorm: 3.638
  Batch   20/48 | Loss: 2.2679 | LR: 4.90e-04 | GradNorm: 3.271
  Batch   24/48 | Loss: 2.4256 | LR: 4.88e-04 | GradNorm: 6.814
  Batch   28/48 | Loss: 2.2233 | LR: 4.86e-04 | GradNorm: 5.065
  Batch   32/48 | Loss: 2.2988 | LR: 4.84e-04 | GradNorm: 5.891
  Batch   36/48 | Loss: 2.2230 | LR: 4.82e-04 | GradNorm: 5.290
  Batch   40/48 | Loss: 2.2519 | LR: 4.80e-04 | GradNorm: 4.200
  Batch   44/48 | Loss: 2.2072 | LR: 4.78e-04 | GradNorm: 4.709
Epoch 11/30: Train=47.62%, Val=35.65%, Best=35.65% (E11), Gap=11.97%, LR=4.77e-04
  Batch    0/48 | Loss: 2.1724 | LR: 4.76e-04 | GradNorm: 3.805
  Batch    4/48 | Loss: 2.2615 | LR: 4.74e-04 | GradNorm: 4.403
  Batch    8/48 | Loss: 2.1712 | LR: 4.72e-04 | GradNorm: 3.590
  Batch   12/48 | Loss: 2.1990 | LR: 4.70e-04 | GradNorm: 4.560
  Batch   16/48 | Loss: 2.1981 | LR: 4.68e-04 | GradNorm: 4.685
  Batch   20/48 | Loss: 2.1798 | LR: 4.66e-04 | GradNorm: 4.257
  Batch   24/48 | Loss: 2.1937 | LR: 4.64e-04 | GradNorm: 4.719
  Batch   28/48 | Loss: 2.1992 | LR: 4.62e-04 | GradNorm: 5.071
  Batch   32/48 | Loss: 2.1785 | LR: 4.60e-04 | GradNorm: 8.295
  Batch   36/48 | Loss: 2.1009 | LR: 4.58e-04 | GradNorm: 4.205
  Batch   40/48 | Loss: 2.0945 | LR: 4.55e-04 | GradNorm: 2.913
  Batch   44/48 | Loss: 2.1297 | LR: 4.53e-04 | GradNorm: 4.172
‚ö†Ô∏è  Below trajectory threshold at epoch 12: 36.06% < 80.0% (Count: 1/3)
Epoch 12/30: Train=50.72%, Val=36.06%, Best=36.06% (E12), Gap=14.66%, LR=4.52e-04
  Batch    0/48 | Loss: 2.1579 | LR: 4.51e-04 | GradNorm: 5.237
  Batch    4/48 | Loss: 2.1210 | LR: 4.49e-04 | GradNorm: 3.641
  Batch    8/48 | Loss: 2.0273 | LR: 4.47e-04 | GradNorm: 3.859
  Batch   12/48 | Loss: 2.0563 | LR: 4.44e-04 | GradNorm: 3.813
  Batch   16/48 | Loss: 2.1049 | LR: 4.42e-04 | GradNorm: 4.444
  Batch   20/48 | Loss: 2.0614 | LR: 4.40e-04 | GradNorm: 4.043
  Batch   24/48 | Loss: 2.1731 | LR: 4.37e-04 | GradNorm: 4.739
  Batch   28/48 | Loss: 2.1125 | LR: 4.35e-04 | GradNorm: 4.526
  Batch   32/48 | Loss: 2.0735 | LR: 4.33e-04 | GradNorm: 4.613
  Batch   36/48 | Loss: 2.0360 | LR: 4.30e-04 | GradNorm: 3.055
  Batch   40/48 | Loss: 2.0758 | LR: 4.28e-04 | GradNorm: 3.892
  Batch   44/48 | Loss: 2.0365 | LR: 4.25e-04 | GradNorm: 4.344
‚ö†Ô∏è  Below trajectory threshold at epoch 13: 39.45% < 80.0% (Count: 2/3)
Epoch 13/30: Train=53.43%, Val=39.45%, Best=39.45% (E13), Gap=13.98%, LR=4.24e-04
  Batch    0/48 | Loss: 2.1182 | LR: 4.23e-04 | GradNorm: 6.309
  Batch    4/48 | Loss: 2.0838 | LR: 4.21e-04 | GradNorm: 4.938
  Batch    8/48 | Loss: 2.0921 | LR: 4.18e-04 | GradNorm: 3.794
  Batch   12/48 | Loss: 2.0511 | LR: 4.16e-04 | GradNorm: 5.263
  Batch   16/48 | Loss: 2.0348 | LR: 4.13e-04 | GradNorm: 3.394
  Batch   20/48 | Loss: 1.9609 | LR: 4.11e-04 | GradNorm: 4.696
  Batch   24/48 | Loss: 1.9825 | LR: 4.08e-04 | GradNorm: 2.914
  Batch   28/48 | Loss: 2.0187 | LR: 4.06e-04 | GradNorm: 4.336
  Batch   32/48 | Loss: 1.9633 | LR: 4.03e-04 | GradNorm: 3.680
  Batch   36/48 | Loss: 1.9556 | LR: 4.00e-04 | GradNorm: 3.819
  Batch   40/48 | Loss: 1.9908 | LR: 3.98e-04 | GradNorm: 4.530
  Batch   44/48 | Loss: 1.9608 | LR: 3.95e-04 | GradNorm: 4.677
‚ö†Ô∏è  Below trajectory threshold at epoch 14: 38.76% < 80.0% (Count: 3/3)
üõë Early stopping: Not following successful LAMB trajectory
   Expected: >80.0% by epoch 12, Got: 38.76%
Final best validation accuracy: 39.45% (epoch 13)
