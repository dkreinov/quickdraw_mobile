=== LAMB Sweep Run ===
LR: 0.000300
Weight Decay: 0.0500
Warmup Epochs: 3
Using device: cuda
Available GPUs: 4
⏱️  Creating train dataset...
Loading QuickDraw dataset from: ../data/quickdraw_parquet
Available classes: 344
Using 50 classes: ['aircraft carrier', 'arm', 'asparagus', 'backpack', 'banana', 'basketball', 'bottlecap', 'bread', 'broom', 'bulldozer']...
Using per-class Parquet format (recommended)...
  Loading 50 classes concurrently from per-class files...
    aircraft carrier: 1200 samples
    arm: 1200 samples
    asparagus: 1200 samples
    backpack: 1200 samples
    banana: 1200 samples
    basketball: 1200 samples
    bottlecap: 1200 samples
    bread: 1200 samples
    broom: 1200 samples
    bulldozer: 1200 samples
    butterfly: 1200 samples
    camel: 1200 samples
    canoe: 1200 samples
    chair: 1200 samples
    compass: 1200 samples
    cookie: 1200 samples
    drums: 1200 samples
    eyeglasses: 1200 samples
    face: 1200 samples
    fan: 1200 samples
    fence: 1200 samples
    fish: 1200 samples
    flying saucer: 1200 samples
    grapes: 1200 samples
    hand: 1200 samples
    hat: 1200 samples
    horse: 1200 samples
    light bulb: 1200 samples
    lighthouse: 1200 samples
    line: 1200 samples
    marker: 1200 samples
    mountain: 1200 samples
    mouse: 1200 samples
    parachute: 1200 samples
    passport: 1200 samples
    pliers: 1200 samples
    potato: 1200 samples
    sea turtle: 1200 samples
    snowflake: 1200 samples
    spider: 1200 samples
    square: 1200 samples
    steak: 1200 samples
    swing set: 1200 samples
    sword: 1200 samples
    television: 1200 samples
    tennis racquet: 1200 samples
    toothbrush: 1200 samples
    train: 1200 samples
    umbrella: 1200 samples
    washing machine: 1200 samples
  Per-class loading complete (concurrent): 60000 samples
Total samples: 60000
⏱️  Train dataset creation took: 0.46s
⏱️  Creating val dataset...
Loading QuickDraw dataset from: ../data/quickdraw_parquet
Available classes: 344
Using 50 classes: ['aircraft carrier', 'arm', 'asparagus', 'backpack', 'banana', 'basketball', 'bottlecap', 'bread', 'broom', 'bulldozer']...
Using per-class Parquet format (recommended)...
  Loading 50 classes concurrently from per-class files...
    aircraft carrier: 1200 samples
    arm: 1200 samples
    asparagus: 1200 samples
    backpack: 1200 samples
    banana: 1200 samples
    basketball: 1200 samples
    bottlecap: 1200 samples
    bread: 1200 samples
    broom: 1200 samples
    bulldozer: 1200 samples
    butterfly: 1200 samples
    camel: 1200 samples
    canoe: 1200 samples
    chair: 1200 samples
    compass: 1200 samples
    cookie: 1200 samples
    drums: 1200 samples
    eyeglasses: 1200 samples
    face: 1200 samples
    fan: 1200 samples
    fence: 1200 samples
    fish: 1200 samples
    flying saucer: 1200 samples
    grapes: 1200 samples
    hand: 1200 samples
    hat: 1200 samples
    horse: 1200 samples
    light bulb: 1200 samples
    lighthouse: 1200 samples
    line: 1200 samples
    marker: 1200 samples
    mountain: 1200 samples
    mouse: 1200 samples
    parachute: 1200 samples
    passport: 1200 samples
    pliers: 1200 samples
    potato: 1200 samples
    sea turtle: 1200 samples
    snowflake: 1200 samples
    spider: 1200 samples
    square: 1200 samples
    steak: 1200 samples
    swing set: 1200 samples
    sword: 1200 samples
    television: 1200 samples
    tennis racquet: 1200 samples
    toothbrush: 1200 samples
    train: 1200 samples
    umbrella: 1200 samples
    washing machine: 1200 samples
  Per-class loading complete (concurrent): 60000 samples
Total samples: 60000
⏱️  Val dataset creation took: 0.44s

Loading train/val split...
Using pre-computed split (fast loading)
⏱️  Split loading took: 0.01s
⏱️  Subset creation took: 0.00s
⏱️  DataLoader creation took: 0.00s
⏱️  TOTAL data loading time: 0.02s

Dataloaders created:
  Training: 48 batches (50000 samples)
  Validation: 10 batches (10000 samples)
Dataset: 50 classes, 48 train batches

Building model:
   Architecture: vit_tiny_patch16_224
   Input channels: 1 (grayscale)
   Output classes: 50
Creating vit_tiny_patch16_224 with 50 classes
   Single-channel input: 1 → 3 channel conversion
   Pretrained: False
   Drop path rate: 0.1
   Random initialization for single-channel weights

Model info:
   Total parameters: 5,435,762
   Trainable parameters: 5,435,762
   Model size: 20.74 MB
Using 4 GPUs with LAMB optimizer
Setting up deterministic training (seed=42)
Created lamb (fallback to AdamW) optimizer:
   Learning rate: 0.000300
   Weight decay: 0.05
   Decay params: 52
   No-decay params: 100
Created learning rate scheduler (step-based):
   Warmup steps: 144
   Total steps: 1440
   Steps per epoch: 48
Created loss function:
   Type: CrossEntropyLoss
   Label smoothing: 0.1
Created AMP gradient scaler
Trainer initialized:
  Model: lamb_lr_0.000300_wd_0.0500
  Train batches: 48
  Val batches: 10
  Save directory: experiments/wandb_lamb_runs
Starting LAMB training for 30 epochs...
/localdrive/users/dkreinov/quickdraw-mobilevit-quant/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:233.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/localdrive/users/dkreinov/quickdraw-mobilevit-quant/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: Memory Efficient attention defaults to a non-deterministic algorithm. To explicitly enable determinism call torch.use_deterministic_algorithms(True, warn_only=False). (Triggered internally at /pytorch/aten/src/ATen/native/transformers/cuda/attention_backward.cu:775.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  Batch    0/48 | Loss: 3.9530 | LR: 5.06e-06 | GradNorm: 1.510
  Batch    4/48 | Loss: 3.9307 | LR: 1.33e-05 | GradNorm: 1.071
  Batch    8/48 | Loss: 3.9222 | LR: 2.16e-05 | GradNorm: 0.914
  Batch   12/48 | Loss: 3.9094 | LR: 2.98e-05 | GradNorm: 0.790
  Batch   16/48 | Loss: 3.9169 | LR: 3.81e-05 | GradNorm: 0.721
  Batch   20/48 | Loss: 3.9088 | LR: 4.63e-05 | GradNorm: 0.732
  Batch   24/48 | Loss: 3.9199 | LR: 5.46e-05 | GradNorm: 0.851
  Batch   28/48 | Loss: 3.9149 | LR: 6.28e-05 | GradNorm: 0.670
  Batch   32/48 | Loss: 3.9016 | LR: 7.11e-05 | GradNorm: 0.546
  Batch   36/48 | Loss: 3.9021 | LR: 7.93e-05 | GradNorm: 0.629
  Batch   40/48 | Loss: 3.8873 | LR: 8.76e-05 | GradNorm: 0.748
  Batch   44/48 | Loss: 3.8530 | LR: 9.58e-05 | GradNorm: 1.749
Epoch 1/30: Train=2.47%, Val=3.26%, Best=3.26% (E1), Gap=-0.79%, LR=1.02e-04
  Batch    0/48 | Loss: 3.8896 | LR: 1.04e-04 | GradNorm: 7.172
  Batch    4/48 | Loss: 3.8342 | LR: 1.12e-04 | GradNorm: 5.323
  Batch    8/48 | Loss: 3.8056 | LR: 1.21e-04 | GradNorm: 4.423
  Batch   12/48 | Loss: 3.7842 | LR: 1.29e-04 | GradNorm: 4.941
  Batch   16/48 | Loss: 3.8456 | LR: 1.37e-04 | GradNorm: 5.262
  Batch   20/48 | Loss: 3.7732 | LR: 1.45e-04 | GradNorm: 2.324
  Batch   24/48 | Loss: 3.7602 | LR: 1.54e-04 | GradNorm: 3.607
  Batch   28/48 | Loss: 3.7820 | LR: 1.62e-04 | GradNorm: 4.125
  Batch   32/48 | Loss: 3.7250 | LR: 1.70e-04 | GradNorm: 3.320
  Batch   36/48 | Loss: 3.7076 | LR: 1.78e-04 | GradNorm: 3.182
  Batch   40/48 | Loss: 3.7434 | LR: 1.87e-04 | GradNorm: 4.257
  Batch   44/48 | Loss: 3.6562 | LR: 1.95e-04 | GradNorm: 1.874
Epoch 2/30: Train=5.73%, Val=7.19%, Best=7.19% (E2), Gap=-1.46%, LR=2.01e-04
  Batch    0/48 | Loss: 3.6433 | LR: 2.03e-04 | GradNorm: 4.399
  Batch    4/48 | Loss: 3.6382 | LR: 2.11e-04 | GradNorm: 3.616
  Batch    8/48 | Loss: 3.7055 | LR: 2.20e-04 | GradNorm: 9.433
