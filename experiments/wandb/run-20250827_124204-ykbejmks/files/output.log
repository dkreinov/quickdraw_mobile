=== LAMB Sweep Run ===
LR: 0.000300
Weight Decay: 0.0500
Warmup Epochs: 2
Using device: cuda
Available GPUs: 4
â±ï¸  Creating train dataset...
Loading QuickDraw dataset from: ../data/quickdraw_parquet
Available classes: 344
Using 50 classes: ['aircraft carrier', 'arm', 'asparagus', 'backpack', 'banana', 'basketball', 'bottlecap', 'bread', 'broom', 'bulldozer']...
Using per-class Parquet format (recommended)...
  Loading 50 classes concurrently from per-class files...
    aircraft carrier: 1200 samples
    arm: 1200 samples
    asparagus: 1200 samples
    backpack: 1200 samples
    banana: 1200 samples
    basketball: 1200 samples
    bottlecap: 1200 samples
    bread: 1200 samples
    broom: 1200 samples
    bulldozer: 1200 samples
    butterfly: 1200 samples
    camel: 1200 samples
    canoe: 1200 samples
    chair: 1200 samples
    compass: 1200 samples
    cookie: 1200 samples
    drums: 1200 samples
    eyeglasses: 1200 samples
    face: 1200 samples
    fan: 1200 samples
    fence: 1200 samples
    fish: 1200 samples
    flying saucer: 1200 samples
    grapes: 1200 samples
    hand: 1200 samples
    hat: 1200 samples
    horse: 1200 samples
    light bulb: 1200 samples
    lighthouse: 1200 samples
    line: 1200 samples
    marker: 1200 samples
    mountain: 1200 samples
    mouse: 1200 samples
    parachute: 1200 samples
    passport: 1200 samples
    pliers: 1200 samples
    potato: 1200 samples
    sea turtle: 1200 samples
    snowflake: 1200 samples
    spider: 1200 samples
    square: 1200 samples
    steak: 1200 samples
    swing set: 1200 samples
    sword: 1200 samples
    television: 1200 samples
    tennis racquet: 1200 samples
    toothbrush: 1200 samples
    train: 1200 samples
    umbrella: 1200 samples
    washing machine: 1200 samples
  Per-class loading complete (concurrent): 60000 samples
Total samples: 60000
â±ï¸  Train dataset creation took: 0.47s
â±ï¸  Creating val dataset...
Loading QuickDraw dataset from: ../data/quickdraw_parquet
Available classes: 344
Using 50 classes: ['aircraft carrier', 'arm', 'asparagus', 'backpack', 'banana', 'basketball', 'bottlecap', 'bread', 'broom', 'bulldozer']...
Using per-class Parquet format (recommended)...
  Loading 50 classes concurrently from per-class files...
    aircraft carrier: 1200 samples
    arm: 1200 samples
    asparagus: 1200 samples
    backpack: 1200 samples
    banana: 1200 samples
    basketball: 1200 samples
    bottlecap: 1200 samples
    bread: 1200 samples
    broom: 1200 samples
    bulldozer: 1200 samples
    butterfly: 1200 samples
    camel: 1200 samples
    canoe: 1200 samples
    chair: 1200 samples
    compass: 1200 samples
    cookie: 1200 samples
    drums: 1200 samples
    eyeglasses: 1200 samples
    face: 1200 samples
    fan: 1200 samples
    fence: 1200 samples
    fish: 1200 samples
    flying saucer: 1200 samples
    grapes: 1200 samples
    hand: 1200 samples
    hat: 1200 samples
    horse: 1200 samples
    light bulb: 1200 samples
    lighthouse: 1200 samples
    line: 1200 samples
    marker: 1200 samples
    mountain: 1200 samples
    mouse: 1200 samples
    parachute: 1200 samples
    passport: 1200 samples
    pliers: 1200 samples
    potato: 1200 samples
    sea turtle: 1200 samples
    snowflake: 1200 samples
    spider: 1200 samples
    square: 1200 samples
    steak: 1200 samples
    swing set: 1200 samples
    sword: 1200 samples
    television: 1200 samples
    tennis racquet: 1200 samples
    toothbrush: 1200 samples
    train: 1200 samples
    umbrella: 1200 samples
    washing machine: 1200 samples
  Per-class loading complete (concurrent): 60000 samples
Total samples: 60000
â±ï¸  Val dataset creation took: 0.43s

Loading train/val split...
Using pre-computed split (fast loading)
â±ï¸  Split loading took: 0.01s
â±ï¸  Subset creation took: 0.00s
â±ï¸  DataLoader creation took: 0.00s
â±ï¸  TOTAL data loading time: 0.02s

Dataloaders created:
  Training: 48 batches (50000 samples)
  Validation: 10 batches (10000 samples)
Dataset: 50 classes, 48 train batches

Building model:
   Architecture: vit_tiny_patch16_224
   Input channels: 1 (grayscale)
   Output classes: 50
Creating vit_tiny_patch16_224 with 50 classes
   Single-channel input: 1 â†’ 3 channel conversion
   Pretrained: True
   Drop path rate: 0.1
   Initialized single-channel weights from pretrained RGB weights

Model info:
   Total parameters: 5,435,762
   Trainable parameters: 5,435,762
   Model size: 20.74 MB
Using 4 GPUs with LAMB optimizer
ðŸ” Debug Info:
   Optimizer requested: lamb
   âœ… LAMB import successful in wandb script
Setting up deterministic training (seed=42)
   TrainingConfig optimizer_name: lamb
   CUSTOM_OPTIMIZERS_AVAILABLE in TrainingConfig: True
Created LAMB optimizer:
   Learning rate: 0.000300
   Weight decay: 0.05
   Optimizer type: LAMB
Created learning rate scheduler (step-based):
   Warmup steps: 96
   Total steps: 1440
   Steps per epoch: 48
Created loss function:
   Type: CrossEntropyLoss
   Label smoothing: 0.1
Created AMP gradient scaler
Trainer initialized:
  Model: lamb_lr_0.000300_wd_0.0500
  Train batches: 48
  Val batches: 10
  Save directory: experiments/wandb_lamb_runs
Starting LAMB training for 30 epochs...
/localdrive/users/dkreinov/quickdraw-mobilevit-quant/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:233.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/localdrive/users/dkreinov/quickdraw-mobilevit-quant/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: Memory Efficient attention defaults to a non-deterministic algorithm. To explicitly enable determinism call torch.use_deterministic_algorithms(True, warn_only=False). (Triggered internally at /pytorch/aten/src/ATen/native/transformers/cuda/attention_backward.cu:775.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  Batch    0/48 | Loss: 4.3255 | LR: 6.09e-06 | GradNorm: 7.766
  Batch    4/48 | Loss: 4.2980 | LR: 1.85e-05 | GradNorm: 7.669
  Batch    8/48 | Loss: 4.2557 | LR: 3.08e-05 | GradNorm: 6.454
  Batch   12/48 | Loss: 4.1589 | LR: 4.32e-05 | GradNorm: 5.637
  Batch   16/48 | Loss: 4.1520 | LR: 5.56e-05 | GradNorm: 4.470
  Batch   20/48 | Loss: 4.1042 | LR: 6.80e-05 | GradNorm: 4.128
  Batch   24/48 | Loss: 4.0862 | LR: 8.03e-05 | GradNorm: 3.389
  Batch   28/48 | Loss: 4.0134 | LR: 9.27e-05 | GradNorm: 2.417
  Batch   32/48 | Loss: 3.9987 | LR: 1.05e-04 | GradNorm: 2.508
  Batch   36/48 | Loss: 3.9027 | LR: 1.17e-04 | GradNorm: 2.588
  Batch   40/48 | Loss: 3.8882 | LR: 1.30e-04 | GradNorm: 2.523
  Batch   44/48 | Loss: 3.8526 | LR: 1.42e-04 | GradNorm: 2.413
Epoch 1/30: Train=3.45%, Val=6.16%, Best=6.16% (E1), Gap=-2.71%, LR=1.51e-04
  Batch    0/48 | Loss: 3.7908 | LR: 1.55e-04 | GradNorm: 2.626
  Batch    4/48 | Loss: 3.7287 | LR: 1.67e-04 | GradNorm: 3.076
  Batch    8/48 | Loss: 3.6684 | LR: 1.79e-04 | GradNorm: 3.088
  Batch   12/48 | Loss: 3.5446 | LR: 1.92e-04 | GradNorm: 3.067
  Batch   16/48 | Loss: 3.4979 | LR: 2.04e-04 | GradNorm: 3.345
  Batch   20/48 | Loss: 3.4226 | LR: 2.16e-04 | GradNorm: 3.665
  Batch   24/48 | Loss: 3.3208 | LR: 2.29e-04 | GradNorm: 3.778
  Batch   28/48 | Loss: 3.2062 | LR: 2.41e-04 | GradNorm: 3.969
