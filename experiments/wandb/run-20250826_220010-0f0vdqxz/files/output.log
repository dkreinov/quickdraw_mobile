=== LAMB Sweep Run ===
LR: 0.000916
Weight Decay: 0.1405
Warmup Epochs: 2
Using device: cuda
Available GPUs: 4
‚è±Ô∏è  Creating train dataset...
Loading QuickDraw dataset from: ../data/quickdraw_parquet
Available classes: 344
Using 50 classes: ['aircraft carrier', 'arm', 'asparagus', 'backpack', 'banana', 'basketball', 'bottlecap', 'bread', 'broom', 'bulldozer']...
Using per-class Parquet format (recommended)...
  Loading 50 classes concurrently from per-class files...
    aircraft carrier: 1200 samples
    arm: 1200 samples
    asparagus: 1200 samples
    backpack: 1200 samples
    banana: 1200 samples
    basketball: 1200 samples
    bottlecap: 1200 samples
    bread: 1200 samples
    broom: 1200 samples
    bulldozer: 1200 samples
    butterfly: 1200 samples
    camel: 1200 samples
    canoe: 1200 samples
    chair: 1200 samples
    compass: 1200 samples
    cookie: 1200 samples
    drums: 1200 samples
    eyeglasses: 1200 samples
    face: 1200 samples
    fan: 1200 samples
    fence: 1200 samples
    fish: 1200 samples
    flying saucer: 1200 samples
    grapes: 1200 samples
    hand: 1200 samples
    hat: 1200 samples
    horse: 1200 samples
    light bulb: 1200 samples
    lighthouse: 1200 samples
    line: 1200 samples
    marker: 1200 samples
    mountain: 1200 samples
    mouse: 1200 samples
    parachute: 1200 samples
    passport: 1200 samples
    pliers: 1200 samples
    potato: 1200 samples
    sea turtle: 1200 samples
    snowflake: 1200 samples
    spider: 1200 samples
    square: 1200 samples
    steak: 1200 samples
    swing set: 1200 samples
    sword: 1200 samples
    television: 1200 samples
    tennis racquet: 1200 samples
    toothbrush: 1200 samples
    train: 1200 samples
    umbrella: 1200 samples
    washing machine: 1200 samples
  Per-class loading complete (concurrent): 60000 samples
Total samples: 60000
‚è±Ô∏è  Train dataset creation took: 0.46s
‚è±Ô∏è  Creating val dataset...
Loading QuickDraw dataset from: ../data/quickdraw_parquet
Available classes: 344
Using 50 classes: ['aircraft carrier', 'arm', 'asparagus', 'backpack', 'banana', 'basketball', 'bottlecap', 'bread', 'broom', 'bulldozer']...
Using per-class Parquet format (recommended)...
  Loading 50 classes concurrently from per-class files...
    aircraft carrier: 1200 samples
    arm: 1200 samples
    asparagus: 1200 samples
    backpack: 1200 samples
    banana: 1200 samples
    basketball: 1200 samples
    bottlecap: 1200 samples
    bread: 1200 samples
    broom: 1200 samples
    bulldozer: 1200 samples
    butterfly: 1200 samples
    camel: 1200 samples
    canoe: 1200 samples
    chair: 1200 samples
    compass: 1200 samples
    cookie: 1200 samples
    drums: 1200 samples
    eyeglasses: 1200 samples
    face: 1200 samples
    fan: 1200 samples
    fence: 1200 samples
    fish: 1200 samples
    flying saucer: 1200 samples
    grapes: 1200 samples
    hand: 1200 samples
    hat: 1200 samples
    horse: 1200 samples
    light bulb: 1200 samples
    lighthouse: 1200 samples
    line: 1200 samples
    marker: 1200 samples
    mountain: 1200 samples
    mouse: 1200 samples
    parachute: 1200 samples
    passport: 1200 samples
    pliers: 1200 samples
    potato: 1200 samples
    sea turtle: 1200 samples
    snowflake: 1200 samples
    spider: 1200 samples
    square: 1200 samples
    steak: 1200 samples
    swing set: 1200 samples
    sword: 1200 samples
    television: 1200 samples
    tennis racquet: 1200 samples
    toothbrush: 1200 samples
    train: 1200 samples
    umbrella: 1200 samples
    washing machine: 1200 samples
  Per-class loading complete (concurrent): 60000 samples
Total samples: 60000
‚è±Ô∏è  Val dataset creation took: 0.42s

Loading train/val split...
Using pre-computed split (fast loading)
‚è±Ô∏è  Split loading took: 0.01s
‚è±Ô∏è  Subset creation took: 0.00s
‚è±Ô∏è  DataLoader creation took: 0.00s
‚è±Ô∏è  TOTAL data loading time: 0.02s

Dataloaders created:
  Training: 48 batches (50000 samples)
  Validation: 10 batches (10000 samples)
Dataset: 50 classes, 48 train batches

Building model:
   Architecture: vit_tiny_patch16_224
   Input channels: 1 (grayscale)
   Output classes: 50
Creating vit_tiny_patch16_224 with 50 classes
   Single-channel input: 1 ‚Üí 3 channel conversion
   Pretrained: False
   Drop path rate: 0.1
   Random initialization for single-channel weights

Model info:
   Total parameters: 5,435,762
   Trainable parameters: 5,435,762
   Model size: 20.74 MB
Using 4 GPUs with LAMB optimizer
Setting up deterministic training (seed=42)
Created lamb (fallback to AdamW) optimizer:
   Learning rate: 0.000916
   Weight decay: 0.14045557776628576
   Decay params: 52
   No-decay params: 100
Created learning rate scheduler (step-based):
   Warmup steps: 96
   Total steps: 1440
   Steps per epoch: 48
Created loss function:
   Type: CrossEntropyLoss
   Label smoothing: 0.1
Created AMP gradient scaler
Trainer initialized:
  Model: lamb_lr_0.000916_wd_0.1405
  Train batches: 48
  Val batches: 10
  Save directory: experiments/wandb_lamb_runs
Starting LAMB training for 30 epochs...
/localdrive/users/dkreinov/quickdraw-mobilevit-quant/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:233.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/localdrive/users/dkreinov/quickdraw-mobilevit-quant/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: Memory Efficient attention defaults to a non-deterministic algorithm. To explicitly enable determinism call torch.use_deterministic_algorithms(True, warn_only=False). (Triggered internally at /pytorch/aten/src/ATen/native/transformers/cuda/attention_backward.cu:775.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  Batch    0/48 | Loss: 3.9690 | LR: 1.86e-05 | GradNorm: 1.932
  Batch    4/48 | Loss: 3.9224 | LR: 5.64e-05 | GradNorm: 0.944
  Batch    8/48 | Loss: 3.9187 | LR: 9.42e-05 | GradNorm: 0.662
  Batch   12/48 | Loss: 3.9138 | LR: 1.32e-04 | GradNorm: 0.544
  Batch   16/48 | Loss: 3.9186 | LR: 1.70e-04 | GradNorm: 0.498
  Batch   20/48 | Loss: 3.9173 | LR: 2.08e-04 | GradNorm: 0.538
  Batch   24/48 | Loss: 3.9160 | LR: 2.45e-04 | GradNorm: 0.596
  Batch   28/48 | Loss: 3.9091 | LR: 2.83e-04 | GradNorm: 0.533
  Batch   32/48 | Loss: 3.8770 | LR: 3.21e-04 | GradNorm: 0.542
  Batch   36/48 | Loss: 3.9524 | LR: 3.59e-04 | GradNorm: 3.883
  Batch   40/48 | Loss: 3.8083 | LR: 3.97e-04 | GradNorm: 1.035
  Batch   44/48 | Loss: 3.8279 | LR: 4.34e-04 | GradNorm: 2.196
Epoch 1/30: Train=2.96%, Val=6.57%, Best=6.57% (E1), Gap=-3.61%, LR=4.63e-04
  Batch    0/48 | Loss: 3.7917 | LR: 4.72e-04 | GradNorm: 1.623
  Batch    4/48 | Loss: 3.7595 | LR: 5.10e-04 | GradNorm: 1.865
  Batch    8/48 | Loss: 3.7383 | LR: 5.48e-04 | GradNorm: 2.362
  Batch   12/48 | Loss: 3.6632 | LR: 5.86e-04 | GradNorm: 2.125
  Batch   16/48 | Loss: 3.9479 | LR: 6.23e-04 | GradNorm: 4.041
  Batch   20/48 | Loss: 3.7569 | LR: 6.61e-04 | GradNorm: 2.785
  Batch   24/48 | Loss: 3.6310 | LR: 6.99e-04 | GradNorm: 1.239
  Batch   28/48 | Loss: 3.7481 | LR: 7.37e-04 | GradNorm: 6.696
  Batch   32/48 | Loss: 3.7802 | LR: 7.75e-04 | GradNorm: 3.742
  Batch   36/48 | Loss: 3.8704 | LR: 8.12e-04 | GradNorm: 5.681
  Batch   40/48 | Loss: 3.7149 | LR: 8.50e-04 | GradNorm: 1.853
  Batch   44/48 | Loss: 3.6863 | LR: 8.88e-04 | GradNorm: 4.793
/localdrive/users/dkreinov/quickdraw-mobilevit-quant/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:209: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Epoch 2/30: Train=6.49%, Val=3.80%, Best=6.57% (E1), Gap=2.69%, LR=9.16e-04
  Batch    0/48 | Loss: 3.8725 | LR: 9.16e-04 | GradNorm: 9.982
  Batch    4/48 | Loss: 3.7849 | LR: 9.16e-04 | GradNorm: 3.841
  Batch    8/48 | Loss: 3.7014 | LR: 9.16e-04 | GradNorm: 1.590
  Batch   12/48 | Loss: 3.6381 | LR: 9.16e-04 | GradNorm: 2.107
  Batch   16/48 | Loss: 3.5561 | LR: 9.16e-04 | GradNorm: 1.131
  Batch   20/48 | Loss: 3.5411 | LR: 9.16e-04 | GradNorm: 2.578
  Batch   24/48 | Loss: 3.6042 | LR: 9.15e-04 | GradNorm: 1.810
  Batch   28/48 | Loss: 3.4861 | LR: 9.15e-04 | GradNorm: 0.854
  Batch   32/48 | Loss: 3.4489 | LR: 9.15e-04 | GradNorm: 0.858
  Batch   36/48 | Loss: 3.4356 | LR: 9.15e-04 | GradNorm: 0.796
  Batch   40/48 | Loss: 3.3854 | LR: 9.14e-04 | GradNorm: 1.332
  Batch   44/48 | Loss: 3.4154 | LR: 9.14e-04 | GradNorm: 3.026
Epoch 3/30: Train=9.77%, Val=6.88%, Best=6.88% (E3), Gap=2.89%, LR=9.13e-04
  Batch    0/48 | Loss: 3.3465 | LR: 9.13e-04 | GradNorm: 2.006
  Batch    4/48 | Loss: 3.4146 | LR: 9.13e-04 | GradNorm: 4.935
  Batch    8/48 | Loss: 3.3371 | LR: 9.12e-04 | GradNorm: 2.502
  Batch   12/48 | Loss: 3.2394 | LR: 9.12e-04 | GradNorm: 1.542
  Batch   16/48 | Loss: 3.4165 | LR: 9.11e-04 | GradNorm: 7.657
  Batch   20/48 | Loss: 3.2128 | LR: 9.10e-04 | GradNorm: 2.136
  Batch   24/48 | Loss: 3.1867 | LR: 9.10e-04 | GradNorm: 1.623
  Batch   28/48 | Loss: 3.2210 | LR: 9.09e-04 | GradNorm: 2.563
  Batch   32/48 | Loss: 3.2193 | LR: 9.08e-04 | GradNorm: 2.143
  Batch   36/48 | Loss: 3.1728 | LR: 9.07e-04 | GradNorm: 1.936
  Batch   40/48 | Loss: 3.2154 | LR: 9.06e-04 | GradNorm: 2.619
  Batch   44/48 | Loss: 3.1512 | LR: 9.05e-04 | GradNorm: 2.599
Epoch 4/30: Train=16.33%, Val=10.19%, Best=10.19% (E4), Gap=6.14%, LR=9.05e-04
  Batch    0/48 | Loss: 3.1445 | LR: 9.05e-04 | GradNorm: 2.790
  Batch    4/48 | Loss: 3.0939 | LR: 9.04e-04 | GradNorm: 1.899
  Batch    8/48 | Loss: 3.1936 | LR: 9.03e-04 | GradNorm: 5.145
  Batch   12/48 | Loss: 3.1525 | LR: 9.01e-04 | GradNorm: 3.234
  Batch   16/48 | Loss: 3.2404 | LR: 9.00e-04 | GradNorm: 2.763
  Batch   20/48 | Loss: 3.2447 | LR: 8.99e-04 | GradNorm: 2.863
  Batch   24/48 | Loss: 3.2386 | LR: 8.98e-04 | GradNorm: 3.333
  Batch   28/48 | Loss: 3.2256 | LR: 8.97e-04 | GradNorm: 3.758
  Batch   32/48 | Loss: 3.1215 | LR: 8.96e-04 | GradNorm: 1.780
  Batch   36/48 | Loss: 3.2540 | LR: 8.94e-04 | GradNorm: 3.780
  Batch   40/48 | Loss: 3.1784 | LR: 8.93e-04 | GradNorm: 1.646
  Batch   44/48 | Loss: 3.1332 | LR: 8.92e-04 | GradNorm: 1.557
Epoch 5/30: Train=17.94%, Val=13.17%, Best=13.17% (E5), Gap=4.77%, LR=8.91e-04
  Batch    0/48 | Loss: 3.0631 | LR: 8.90e-04 | GradNorm: 1.745
  Batch    4/48 | Loss: 3.0570 | LR: 8.89e-04 | GradNorm: 3.101
  Batch    8/48 | Loss: 3.0904 | LR: 8.87e-04 | GradNorm: 3.550
  Batch   12/48 | Loss: 3.0497 | LR: 8.86e-04 | GradNorm: 1.641
  Batch   16/48 | Loss: 3.0221 | LR: 8.84e-04 | GradNorm: 1.539
  Batch   20/48 | Loss: 3.0303 | LR: 8.83e-04 | GradNorm: 2.475
  Batch   24/48 | Loss: 3.0875 | LR: 8.81e-04 | GradNorm: 4.309
  Batch   28/48 | Loss: 3.0184 | LR: 8.79e-04 | GradNorm: 3.970
  Batch   32/48 | Loss: 2.9610 | LR: 8.78e-04 | GradNorm: 1.935
  Batch   36/48 | Loss: 2.9759 | LR: 8.76e-04 | GradNorm: 1.870
  Batch   40/48 | Loss: 2.9680 | LR: 8.74e-04 | GradNorm: 2.986
  Batch   44/48 | Loss: 3.0173 | LR: 8.72e-04 | GradNorm: 2.235
Epoch 6/30: Train=22.69%, Val=14.02%, Best=14.02% (E6), Gap=8.67%, LR=8.71e-04
  Batch    0/48 | Loss: 3.0018 | LR: 8.70e-04 | GradNorm: 4.438
  Batch    4/48 | Loss: 2.8924 | LR: 8.69e-04 | GradNorm: 2.634
  Batch    8/48 | Loss: 2.8930 | LR: 8.67e-04 | GradNorm: 1.746
  Batch   12/48 | Loss: 2.8370 | LR: 8.65e-04 | GradNorm: 2.614
  Batch   16/48 | Loss: 2.9600 | LR: 8.63e-04 | GradNorm: 5.860
  Batch   20/48 | Loss: 2.9048 | LR: 8.61e-04 | GradNorm: 2.617
  Batch   24/48 | Loss: 3.0283 | LR: 8.59e-04 | GradNorm: 4.798
  Batch   28/48 | Loss: 2.8645 | LR: 8.57e-04 | GradNorm: 1.896
  Batch   32/48 | Loss: 2.8977 | LR: 8.54e-04 | GradNorm: 3.917
  Batch   36/48 | Loss: 2.8739 | LR: 8.52e-04 | GradNorm: 1.922
  Batch   40/48 | Loss: 2.8864 | LR: 8.50e-04 | GradNorm: 3.027
  Batch   44/48 | Loss: 2.8008 | LR: 8.48e-04 | GradNorm: 1.925
Epoch 7/30: Train=26.77%, Val=19.77%, Best=19.77% (E7), Gap=7.00%, LR=8.46e-04
  Batch    0/48 | Loss: 2.7844 | LR: 8.45e-04 | GradNorm: 1.930
  Batch    4/48 | Loss: 2.7764 | LR: 8.43e-04 | GradNorm: 3.573
  Batch    8/48 | Loss: 2.6988 | LR: 8.41e-04 | GradNorm: 2.197
  Batch   12/48 | Loss: 2.7865 | LR: 8.38e-04 | GradNorm: 3.005
  Batch   16/48 | Loss: 2.8046 | LR: 8.36e-04 | GradNorm: 4.266
  Batch   20/48 | Loss: 2.6978 | LR: 8.34e-04 | GradNorm: 3.851
  Batch   24/48 | Loss: 2.6597 | LR: 8.31e-04 | GradNorm: 2.096
  Batch   28/48 | Loss: 2.7189 | LR: 8.29e-04 | GradNorm: 3.243
  Batch   32/48 | Loss: 2.6930 | LR: 8.26e-04 | GradNorm: 2.683
  Batch   36/48 | Loss: 2.6800 | LR: 8.24e-04 | GradNorm: 4.347
  Batch   40/48 | Loss: 2.6079 | LR: 8.21e-04 | GradNorm: 3.286
  Batch   44/48 | Loss: 2.6892 | LR: 8.18e-04 | GradNorm: 3.077
Epoch 8/30: Train=32.40%, Val=19.24%, Best=19.77% (E7), Gap=13.16%, LR=8.16e-04
  Batch    0/48 | Loss: 2.6338 | LR: 8.16e-04 | GradNorm: 2.535
  Batch    4/48 | Loss: 2.6454 | LR: 8.13e-04 | GradNorm: 3.139
  Batch    8/48 | Loss: 2.6696 | LR: 8.10e-04 | GradNorm: 3.012
  Batch   12/48 | Loss: 2.5979 | LR: 8.07e-04 | GradNorm: 2.671
  Batch   16/48 | Loss: 2.5087 | LR: 8.05e-04 | GradNorm: 1.975
  Batch   20/48 | Loss: 2.6191 | LR: 8.02e-04 | GradNorm: 4.570
  Batch   24/48 | Loss: 2.5754 | LR: 7.99e-04 | GradNorm: 2.563
  Batch   28/48 | Loss: 2.5598 | LR: 7.96e-04 | GradNorm: 1.797
  Batch   32/48 | Loss: 2.5476 | LR: 7.93e-04 | GradNorm: 2.250
  Batch   36/48 | Loss: 2.5459 | LR: 7.90e-04 | GradNorm: 2.950
  Batch   40/48 | Loss: 2.5821 | LR: 7.87e-04 | GradNorm: 3.443
  Batch   44/48 | Loss: 2.4935 | LR: 7.84e-04 | GradNorm: 2.090
Epoch 9/30: Train=35.92%, Val=27.49%, Best=27.49% (E9), Gap=8.43%, LR=7.82e-04
  Batch    0/48 | Loss: 2.5211 | LR: 7.81e-04 | GradNorm: 2.712
  Batch    4/48 | Loss: 2.5025 | LR: 7.78e-04 | GradNorm: 2.931
  Batch    8/48 | Loss: 2.4794 | LR: 7.75e-04 | GradNorm: 3.382
  Batch   12/48 | Loss: 2.5457 | LR: 7.72e-04 | GradNorm: 3.586
  Batch   16/48 | Loss: 2.4711 | LR: 7.69e-04 | GradNorm: 3.124
  Batch   20/48 | Loss: 2.5751 | LR: 7.66e-04 | GradNorm: 4.454
  Batch   24/48 | Loss: 2.4874 | LR: 7.63e-04 | GradNorm: 2.283
  Batch   28/48 | Loss: 2.5227 | LR: 7.59e-04 | GradNorm: 4.019
  Batch   32/48 | Loss: 2.4816 | LR: 7.56e-04 | GradNorm: 2.990
  Batch   36/48 | Loss: 2.4342 | LR: 7.53e-04 | GradNorm: 2.574
  Batch   40/48 | Loss: 2.3653 | LR: 7.50e-04 | GradNorm: 2.710
  Batch   44/48 | Loss: 2.4056 | LR: 7.46e-04 | GradNorm: 3.210
Epoch 10/30: Train=39.44%, Val=28.87%, Best=28.87% (E10), Gap=10.57%, LR=7.44e-04
  Batch    0/48 | Loss: 2.3782 | LR: 7.43e-04 | GradNorm: 2.793
  Batch    4/48 | Loss: 2.3778 | LR: 7.40e-04 | GradNorm: 2.911
  Batch    8/48 | Loss: 2.4095 | LR: 7.36e-04 | GradNorm: 4.026
  Batch   12/48 | Loss: 2.3848 | LR: 7.33e-04 | GradNorm: 1.949
  Batch   16/48 | Loss: 2.3406 | LR: 7.29e-04 | GradNorm: 4.432
  Batch   20/48 | Loss: 2.4096 | LR: 7.26e-04 | GradNorm: 3.779
  Batch   24/48 | Loss: 2.4443 | LR: 7.22e-04 | GradNorm: 2.301
  Batch   28/48 | Loss: 2.3673 | LR: 7.19e-04 | GradNorm: 5.940
  Batch   32/48 | Loss: 2.3986 | LR: 7.15e-04 | GradNorm: 4.205
  Batch   36/48 | Loss: 2.3746 | LR: 7.12e-04 | GradNorm: 3.361
  Batch   40/48 | Loss: 2.3971 | LR: 7.08e-04 | GradNorm: 2.619
  Batch   44/48 | Loss: 2.3364 | LR: 7.05e-04 | GradNorm: 2.773
Epoch 11/30: Train=43.44%, Val=31.70%, Best=31.70% (E11), Gap=11.74%, LR=7.02e-04
  Batch    0/48 | Loss: 2.2931 | LR: 7.01e-04 | GradNorm: 3.354
  Batch    4/48 | Loss: 2.3550 | LR: 6.97e-04 | GradNorm: 1.957
  Batch    8/48 | Loss: 2.3009 | LR: 6.94e-04 | GradNorm: 2.918
  Batch   12/48 | Loss: 2.3153 | LR: 6.90e-04 | GradNorm: 2.531
  Batch   16/48 | Loss: 2.2818 | LR: 6.86e-04 | GradNorm: 3.312
  Batch   20/48 | Loss: 2.2592 | LR: 6.83e-04 | GradNorm: 1.873
  Batch   24/48 | Loss: 2.3286 | LR: 6.79e-04 | GradNorm: 4.046
  Batch   28/48 | Loss: 2.3284 | LR: 6.75e-04 | GradNorm: 4.134
  Batch   32/48 | Loss: 2.2897 | LR: 6.71e-04 | GradNorm: 3.824
  Batch   36/48 | Loss: 2.1738 | LR: 6.67e-04 | GradNorm: 1.904
  Batch   40/48 | Loss: 2.1813 | LR: 6.64e-04 | GradNorm: 1.683
  Batch   44/48 | Loss: 2.1608 | LR: 6.60e-04 | GradNorm: 2.673
‚ö†Ô∏è  Below trajectory threshold at epoch 12: 36.69% < 80.0% (Count: 1/3)
Epoch 12/30: Train=47.00%, Val=36.69%, Best=36.69% (E12), Gap=10.31%, LR=6.57e-04
  Batch    0/48 | Loss: 2.2137 | LR: 6.56e-04 | GradNorm: 1.940
  Batch    4/48 | Loss: 2.1612 | LR: 6.52e-04 | GradNorm: 1.993
  Batch    8/48 | Loss: 2.0676 | LR: 6.48e-04 | GradNorm: 2.161
  Batch   12/48 | Loss: 2.1163 | LR: 6.44e-04 | GradNorm: 3.133
  Batch   16/48 | Loss: 2.1265 | LR: 6.40e-04 | GradNorm: 2.816
  Batch   20/48 | Loss: 2.1619 | LR: 6.36e-04 | GradNorm: 4.301
  Batch   24/48 | Loss: 2.2227 | LR: 6.33e-04 | GradNorm: 2.795
  Batch   28/48 | Loss: 2.1454 | LR: 6.29e-04 | GradNorm: 2.557
  Batch   32/48 | Loss: 2.1317 | LR: 6.25e-04 | GradNorm: 2.864
  Batch   36/48 | Loss: 2.1236 | LR: 6.21e-04 | GradNorm: 2.922
  Batch   40/48 | Loss: 2.1485 | LR: 6.17e-04 | GradNorm: 2.493
  Batch   44/48 | Loss: 2.0688 | LR: 6.13e-04 | GradNorm: 2.169
‚ö†Ô∏è  Below trajectory threshold at epoch 13: 42.41% < 80.0% (Count: 2/3)
Epoch 13/30: Train=51.14%, Val=42.41%, Best=42.41% (E13), Gap=8.73%, LR=6.09e-04
  Batch    0/48 | Loss: 2.1219 | LR: 6.08e-04 | GradNorm: 2.270
  Batch    4/48 | Loss: 2.1030 | LR: 6.04e-04 | GradNorm: 2.939
  Batch    8/48 | Loss: 2.1798 | LR: 6.00e-04 | GradNorm: 3.560
  Batch   12/48 | Loss: 2.0737 | LR: 5.96e-04 | GradNorm: 2.344
  Batch   16/48 | Loss: 2.0526 | LR: 5.92e-04 | GradNorm: 2.099
  Batch   20/48 | Loss: 2.0165 | LR: 5.88e-04 | GradNorm: 2.500
  Batch   24/48 | Loss: 2.1072 | LR: 5.84e-04 | GradNorm: 3.006
  Batch   28/48 | Loss: 2.1282 | LR: 5.80e-04 | GradNorm: 3.823
  Batch   32/48 | Loss: 2.0350 | LR: 5.76e-04 | GradNorm: 2.911
  Batch   36/48 | Loss: 2.0072 | LR: 5.72e-04 | GradNorm: 2.278
  Batch   40/48 | Loss: 2.0427 | LR: 5.67e-04 | GradNorm: 2.023
  Batch   44/48 | Loss: 2.0213 | LR: 5.63e-04 | GradNorm: 2.653
‚ö†Ô∏è  Below trajectory threshold at epoch 14: 43.23% < 80.0% (Count: 3/3)
üõë Early stopping: Not following successful LAMB trajectory
   Expected: >80.0% by epoch 12, Got: 43.23%
Final best validation accuracy: 43.23% (epoch 14)
