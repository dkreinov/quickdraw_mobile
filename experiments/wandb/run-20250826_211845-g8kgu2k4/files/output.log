=== LAMB Sweep Run ===
LR: 0.000919
Weight Decay: 0.1384
Warmup Epochs: 3
Using device: cuda
Available GPUs: 4
‚è±Ô∏è  Creating train dataset...
Loading QuickDraw dataset from: ../data/quickdraw_parquet
Available classes: 344
Using 50 classes: ['aircraft carrier', 'arm', 'asparagus', 'backpack', 'banana', 'basketball', 'bottlecap', 'bread', 'broom', 'bulldozer']...
Using per-class Parquet format (recommended)...
  Loading 50 classes concurrently from per-class files...
    aircraft carrier: 1200 samples
    arm: 1200 samples
    asparagus: 1200 samples
    backpack: 1200 samples
    banana: 1200 samples
    basketball: 1200 samples
    bottlecap: 1200 samples
    bread: 1200 samples
    broom: 1200 samples
    bulldozer: 1200 samples
    butterfly: 1200 samples
    camel: 1200 samples
    canoe: 1200 samples
    chair: 1200 samples
    compass: 1200 samples
    cookie: 1200 samples
    drums: 1200 samples
    eyeglasses: 1200 samples
    face: 1200 samples
    fan: 1200 samples
    fence: 1200 samples
    fish: 1200 samples
    flying saucer: 1200 samples
    grapes: 1200 samples
    hand: 1200 samples
    hat: 1200 samples
    horse: 1200 samples
    light bulb: 1200 samples
    lighthouse: 1200 samples
    line: 1200 samples
    marker: 1200 samples
    mountain: 1200 samples
    mouse: 1200 samples
    parachute: 1200 samples
    passport: 1200 samples
    pliers: 1200 samples
    potato: 1200 samples
    sea turtle: 1200 samples
    snowflake: 1200 samples
    spider: 1200 samples
    square: 1200 samples
    steak: 1200 samples
    swing set: 1200 samples
    sword: 1200 samples
    television: 1200 samples
    tennis racquet: 1200 samples
    toothbrush: 1200 samples
    train: 1200 samples
    umbrella: 1200 samples
    washing machine: 1200 samples
  Per-class loading complete (concurrent): 60000 samples
Total samples: 60000
‚è±Ô∏è  Train dataset creation took: 0.48s
‚è±Ô∏è  Creating val dataset...
Loading QuickDraw dataset from: ../data/quickdraw_parquet
Available classes: 344
Using 50 classes: ['aircraft carrier', 'arm', 'asparagus', 'backpack', 'banana', 'basketball', 'bottlecap', 'bread', 'broom', 'bulldozer']...
Using per-class Parquet format (recommended)...
  Loading 50 classes concurrently from per-class files...
    aircraft carrier: 1200 samples
    arm: 1200 samples
    asparagus: 1200 samples
    backpack: 1200 samples
    banana: 1200 samples
    basketball: 1200 samples
    bottlecap: 1200 samples
    bread: 1200 samples
    broom: 1200 samples
    bulldozer: 1200 samples
    butterfly: 1200 samples
    camel: 1200 samples
    canoe: 1200 samples
    chair: 1200 samples
    compass: 1200 samples
    cookie: 1200 samples
    drums: 1200 samples
    eyeglasses: 1200 samples
    face: 1200 samples
    fan: 1200 samples
    fence: 1200 samples
    fish: 1200 samples
    flying saucer: 1200 samples
    grapes: 1200 samples
    hand: 1200 samples
    hat: 1200 samples
    horse: 1200 samples
    light bulb: 1200 samples
    lighthouse: 1200 samples
    line: 1200 samples
    marker: 1200 samples
    mountain: 1200 samples
    mouse: 1200 samples
    parachute: 1200 samples
    passport: 1200 samples
    pliers: 1200 samples
    potato: 1200 samples
    sea turtle: 1200 samples
    snowflake: 1200 samples
    spider: 1200 samples
    square: 1200 samples
    steak: 1200 samples
    swing set: 1200 samples
    sword: 1200 samples
    television: 1200 samples
    tennis racquet: 1200 samples
    toothbrush: 1200 samples
    train: 1200 samples
    umbrella: 1200 samples
    washing machine: 1200 samples
  Per-class loading complete (concurrent): 60000 samples
Total samples: 60000
‚è±Ô∏è  Val dataset creation took: 0.43s

Loading train/val split...
Using pre-computed split (fast loading)
‚è±Ô∏è  Split loading took: 0.01s
‚è±Ô∏è  Subset creation took: 0.00s
‚è±Ô∏è  DataLoader creation took: 0.00s
‚è±Ô∏è  TOTAL data loading time: 0.02s

Dataloaders created:
  Training: 48 batches (50000 samples)
  Validation: 10 batches (10000 samples)
Dataset: 50 classes, 48 train batches

Building model:
   Architecture: vit_tiny_patch16_224
   Input channels: 1 (grayscale)
   Output classes: 50
Creating vit_tiny_patch16_224 with 50 classes
   Single-channel input: 1 ‚Üí 3 channel conversion
   Pretrained: False
   Drop path rate: 0.1
   Random initialization for single-channel weights

Model info:
   Total parameters: 5,435,762
   Trainable parameters: 5,435,762
   Model size: 20.74 MB
Using 4 GPUs with LAMB optimizer
Setting up deterministic training (seed=42)
Created lamb (fallback to AdamW) optimizer:
   Learning rate: 0.000919
   Weight decay: 0.13844877489587476
   Decay params: 52
   No-decay params: 100
Created learning rate scheduler (step-based):
   Warmup steps: 144
   Total steps: 1440
   Steps per epoch: 48
Created loss function:
   Type: CrossEntropyLoss
   Label smoothing: 0.1
Created AMP gradient scaler
Trainer initialized:
  Model: lamb_lr_0.000919_wd_0.1384
  Train batches: 48
  Val batches: 10
  Save directory: experiments/wandb_lamb_runs
Starting LAMB training for 30 epochs...
/localdrive/users/dkreinov/quickdraw-mobilevit-quant/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:233.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/localdrive/users/dkreinov/quickdraw-mobilevit-quant/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:829: UserWarning: Memory Efficient attention defaults to a non-deterministic algorithm. To explicitly enable determinism call torch.use_deterministic_algorithms(True, warn_only=False). (Triggered internally at /pytorch/aten/src/ATen/native/transformers/cuda/attention_backward.cu:775.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  Batch    0/48 | Loss: 3.9446 | LR: 1.55e-05 | GradNorm: 1.456
  Batch    4/48 | Loss: 3.9239 | LR: 4.08e-05 | GradNorm: 0.929
  Batch    8/48 | Loss: 3.9272 | LR: 6.60e-05 | GradNorm: 0.731
  Batch   12/48 | Loss: 3.9093 | LR: 9.13e-05 | GradNorm: 0.520
  Batch   16/48 | Loss: 3.9225 | LR: 1.17e-04 | GradNorm: 0.572
  Batch   20/48 | Loss: 3.9154 | LR: 1.42e-04 | GradNorm: 0.576
  Batch   24/48 | Loss: 3.9084 | LR: 1.67e-04 | GradNorm: 0.540
  Batch   28/48 | Loss: 3.9153 | LR: 1.92e-04 | GradNorm: 0.564
  Batch   32/48 | Loss: 3.8990 | LR: 2.18e-04 | GradNorm: 0.493
  Batch   36/48 | Loss: 3.8768 | LR: 2.43e-04 | GradNorm: 0.571
  Batch   40/48 | Loss: 3.8308 | LR: 2.68e-04 | GradNorm: 0.771
  Batch   44/48 | Loss: 3.8663 | LR: 2.93e-04 | GradNorm: 3.424
Epoch 1/30: Train=2.93%, Val=3.52%, Best=3.52% (E1), Gap=-0.59%, LR=3.12e-04
  Batch    0/48 | Loss: 3.8669 | LR: 3.19e-04 | GradNorm: 3.991
  Batch    4/48 | Loss: 3.9141 | LR: 3.44e-04 | GradNorm: 2.176
  Batch    8/48 | Loss: 3.8546 | LR: 3.69e-04 | GradNorm: 1.768
  Batch   12/48 | Loss: 3.7471 | LR: 3.95e-04 | GradNorm: 0.845
  Batch   16/48 | Loss: 3.7214 | LR: 4.20e-04 | GradNorm: 0.549
  Batch   20/48 | Loss: 3.9144 | LR: 4.45e-04 | GradNorm: 2.541
  Batch   24/48 | Loss: 3.6837 | LR: 4.70e-04 | GradNorm: 2.073
  Batch   28/48 | Loss: 3.8668 | LR: 4.96e-04 | GradNorm: 3.996
  Batch   32/48 | Loss: 3.7527 | LR: 5.21e-04 | GradNorm: 1.164
  Batch   36/48 | Loss: 3.6970 | LR: 5.46e-04 | GradNorm: 1.695
  Batch   40/48 | Loss: 3.7820 | LR: 5.71e-04 | GradNorm: 2.944
  Batch   44/48 | Loss: 3.7104 | LR: 5.97e-04 | GradNorm: 1.366
Epoch 2/30: Train=5.70%, Val=8.73%, Best=8.73% (E2), Gap=-3.03%, LR=6.16e-04
  Batch    0/48 | Loss: 3.6520 | LR: 6.22e-04 | GradNorm: 1.676
  Batch    4/48 | Loss: 3.6412 | LR: 6.47e-04 | GradNorm: 2.250
  Batch    8/48 | Loss: 3.5782 | LR: 6.72e-04 | GradNorm: 1.448
  Batch   12/48 | Loss: 3.9101 | LR: 6.98e-04 | GradNorm: 3.893
  Batch   16/48 | Loss: 3.7273 | LR: 7.23e-04 | GradNorm: 0.841
  Batch   20/48 | Loss: 3.6781 | LR: 7.48e-04 | GradNorm: 0.999
  Batch   24/48 | Loss: 3.6774 | LR: 7.74e-04 | GradNorm: 3.925
  Batch   28/48 | Loss: 3.5872 | LR: 7.99e-04 | GradNorm: 0.900
  Batch   32/48 | Loss: 3.5567 | LR: 8.24e-04 | GradNorm: 1.087
  Batch   36/48 | Loss: 3.5734 | LR: 8.49e-04 | GradNorm: 1.830
  Batch   40/48 | Loss: 3.4861 | LR: 8.75e-04 | GradNorm: 0.921
  Batch   44/48 | Loss: 3.5207 | LR: 9.00e-04 | GradNorm: 4.373
/localdrive/users/dkreinov/quickdraw-mobilevit-quant/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:209: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Epoch 3/30: Train=8.21%, Val=6.73%, Best=8.73% (E2), Gap=1.48%, LR=9.19e-04
  Batch    0/48 | Loss: 3.5153 | LR: 9.19e-04 | GradNorm: 1.723
  Batch    4/48 | Loss: 3.6754 | LR: 9.19e-04 | GradNorm: 6.843
  Batch    8/48 | Loss: 3.9660 | LR: 9.19e-04 | GradNorm: 5.535
  Batch   12/48 | Loss: 3.7551 | LR: 9.19e-04 | GradNorm: 2.033
  Batch   16/48 | Loss: 3.7464 | LR: 9.18e-04 | GradNorm: 1.279
  Batch   20/48 | Loss: 3.6773 | LR: 9.18e-04 | GradNorm: 1.279
  Batch   24/48 | Loss: 3.6245 | LR: 9.18e-04 | GradNorm: 1.388
  Batch   28/48 | Loss: 3.5334 | LR: 9.18e-04 | GradNorm: 1.257
  Batch   32/48 | Loss: 3.5085 | LR: 9.17e-04 | GradNorm: 2.212
  Batch   36/48 | Loss: 3.4857 | LR: 9.17e-04 | GradNorm: 3.388
  Batch   40/48 | Loss: 3.4214 | LR: 9.17e-04 | GradNorm: 1.291
  Batch   44/48 | Loss: 3.3593 | LR: 9.16e-04 | GradNorm: 1.543
Epoch 4/30: Train=9.73%, Val=9.04%, Best=9.04% (E4), Gap=0.69%, LR=9.16e-04
  Batch    0/48 | Loss: 3.3936 | LR: 9.16e-04 | GradNorm: 3.645
  Batch    4/48 | Loss: 3.3101 | LR: 9.15e-04 | GradNorm: 1.710
  Batch    8/48 | Loss: 3.3245 | LR: 9.14e-04 | GradNorm: 2.902
  Batch   12/48 | Loss: 3.3074 | LR: 9.14e-04 | GradNorm: 4.146
  Batch   16/48 | Loss: 3.3365 | LR: 9.13e-04 | GradNorm: 2.475
  Batch   20/48 | Loss: 3.2694 | LR: 9.12e-04 | GradNorm: 2.312
  Batch   24/48 | Loss: 3.3311 | LR: 9.12e-04 | GradNorm: 3.845
  Batch   28/48 | Loss: 3.2570 | LR: 9.11e-04 | GradNorm: 3.563
  Batch   32/48 | Loss: 3.3909 | LR: 9.10e-04 | GradNorm: 4.152
  Batch   36/48 | Loss: 3.3391 | LR: 9.09e-04 | GradNorm: 1.281
  Batch   40/48 | Loss: 3.3270 | LR: 9.08e-04 | GradNorm: 2.344
  Batch   44/48 | Loss: 3.2864 | LR: 9.07e-04 | GradNorm: 2.016
Epoch 5/30: Train=15.06%, Val=13.17%, Best=13.17% (E5), Gap=1.89%, LR=9.06e-04
  Batch    0/48 | Loss: 3.2655 | LR: 9.06e-04 | GradNorm: 2.496
  Batch    4/48 | Loss: 3.1775 | LR: 9.05e-04 | GradNorm: 1.902
  Batch    8/48 | Loss: 3.1618 | LR: 9.04e-04 | GradNorm: 2.114
  Batch   12/48 | Loss: 3.1698 | LR: 9.03e-04 | GradNorm: 2.010
  Batch   16/48 | Loss: 3.1734 | LR: 9.02e-04 | GradNorm: 1.703
  Batch   20/48 | Loss: 3.1454 | LR: 9.00e-04 | GradNorm: 1.629
  Batch   24/48 | Loss: 3.1137 | LR: 8.99e-04 | GradNorm: 1.845
  Batch   28/48 | Loss: 3.1720 | LR: 8.98e-04 | GradNorm: 2.719
  Batch   32/48 | Loss: 3.1361 | LR: 8.97e-04 | GradNorm: 2.243
  Batch   36/48 | Loss: 3.1515 | LR: 8.95e-04 | GradNorm: 2.350
  Batch   40/48 | Loss: 3.1134 | LR: 8.94e-04 | GradNorm: 2.399
  Batch   44/48 | Loss: 3.1499 | LR: 8.92e-04 | GradNorm: 1.677
Epoch 6/30: Train=18.72%, Val=11.85%, Best=13.17% (E5), Gap=6.87%, LR=8.91e-04
  Batch    0/48 | Loss: 3.1031 | LR: 8.91e-04 | GradNorm: 1.816
  Batch    4/48 | Loss: 3.1046 | LR: 8.89e-04 | GradNorm: 2.382
  Batch    8/48 | Loss: 3.1050 | LR: 8.88e-04 | GradNorm: 2.044
  Batch   12/48 | Loss: 3.0597 | LR: 8.86e-04 | GradNorm: 3.084
  Batch   16/48 | Loss: 3.0825 | LR: 8.84e-04 | GradNorm: 2.261
  Batch   20/48 | Loss: 3.1001 | LR: 8.83e-04 | GradNorm: 4.106
  Batch   24/48 | Loss: 3.0527 | LR: 8.81e-04 | GradNorm: 1.613
  Batch   28/48 | Loss: 3.1375 | LR: 8.79e-04 | GradNorm: 3.011
  Batch   32/48 | Loss: 3.1463 | LR: 8.77e-04 | GradNorm: 4.665
  Batch   36/48 | Loss: 3.0941 | LR: 8.75e-04 | GradNorm: 2.534
  Batch   40/48 | Loss: 3.0943 | LR: 8.73e-04 | GradNorm: 2.206
  Batch   44/48 | Loss: 3.1203 | LR: 8.71e-04 | GradNorm: 2.018
Epoch 7/30: Train=21.31%, Val=13.83%, Best=13.83% (E7), Gap=7.48%, LR=8.70e-04
  Batch    0/48 | Loss: 3.0220 | LR: 8.69e-04 | GradNorm: 1.822
  Batch    4/48 | Loss: 2.9829 | LR: 8.67e-04 | GradNorm: 2.446
  Batch    8/48 | Loss: 3.0046 | LR: 8.65e-04 | GradNorm: 3.930
  Batch   12/48 | Loss: 3.0743 | LR: 8.63e-04 | GradNorm: 3.764
  Batch   16/48 | Loss: 3.0468 | LR: 8.61e-04 | GradNorm: 2.873
  Batch   20/48 | Loss: 2.9935 | LR: 8.59e-04 | GradNorm: 5.700
  Batch   24/48 | Loss: 2.9718 | LR: 8.57e-04 | GradNorm: 2.627
  Batch   28/48 | Loss: 2.9480 | LR: 8.54e-04 | GradNorm: 1.725
  Batch   32/48 | Loss: 2.9617 | LR: 8.52e-04 | GradNorm: 2.708
  Batch   36/48 | Loss: 2.9361 | LR: 8.50e-04 | GradNorm: 2.973
  Batch   40/48 | Loss: 2.8486 | LR: 8.47e-04 | GradNorm: 2.322
  Batch   44/48 | Loss: 2.8884 | LR: 8.45e-04 | GradNorm: 1.414
Epoch 8/30: Train=24.73%, Val=17.41%, Best=17.41% (E8), Gap=7.32%, LR=8.43e-04
  Batch    0/48 | Loss: 2.8493 | LR: 8.43e-04 | GradNorm: 3.409
  Batch    4/48 | Loss: 2.8238 | LR: 8.40e-04 | GradNorm: 1.605
  Batch    8/48 | Loss: 2.9880 | LR: 8.38e-04 | GradNorm: 6.293
  Batch   12/48 | Loss: 2.8837 | LR: 8.35e-04 | GradNorm: 1.835
  Batch   16/48 | Loss: 2.8688 | LR: 8.32e-04 | GradNorm: 3.300
  Batch   20/48 | Loss: 2.8024 | LR: 8.30e-04 | GradNorm: 2.714
  Batch   24/48 | Loss: 2.8372 | LR: 8.27e-04 | GradNorm: 2.220
  Batch   28/48 | Loss: 2.9126 | LR: 8.25e-04 | GradNorm: 7.468
  Batch   32/48 | Loss: 2.8768 | LR: 8.22e-04 | GradNorm: 3.806
  Batch   36/48 | Loss: 2.8275 | LR: 8.19e-04 | GradNorm: 2.547
  Batch   40/48 | Loss: 2.7309 | LR: 8.16e-04 | GradNorm: 1.623
  Batch   44/48 | Loss: 2.7666 | LR: 8.13e-04 | GradNorm: 1.769
Epoch 9/30: Train=28.01%, Val=18.81%, Best=18.81% (E9), Gap=9.20%, LR=8.11e-04
  Batch    0/48 | Loss: 2.8508 | LR: 8.11e-04 | GradNorm: 5.309
  Batch    4/48 | Loss: 2.7862 | LR: 8.08e-04 | GradNorm: 3.267
  Batch    8/48 | Loss: 2.7335 | LR: 8.05e-04 | GradNorm: 2.969
  Batch   12/48 | Loss: 2.7386 | LR: 8.02e-04 | GradNorm: 3.547
  Batch   16/48 | Loss: 2.7122 | LR: 7.99e-04 | GradNorm: 4.099
  Batch   20/48 | Loss: 2.7206 | LR: 7.96e-04 | GradNorm: 3.277
  Batch   24/48 | Loss: 2.7484 | LR: 7.93e-04 | GradNorm: 2.076
  Batch   28/48 | Loss: 2.7042 | LR: 7.90e-04 | GradNorm: 4.187
  Batch   32/48 | Loss: 2.7577 | LR: 7.87e-04 | GradNorm: 5.096
  Batch   36/48 | Loss: 2.7196 | LR: 7.83e-04 | GradNorm: 5.057
  Batch   40/48 | Loss: 2.6219 | LR: 7.80e-04 | GradNorm: 3.218
  Batch   44/48 | Loss: 2.6518 | LR: 7.77e-04 | GradNorm: 4.201
Epoch 10/30: Train=31.94%, Val=22.72%, Best=22.72% (E10), Gap=9.22%, LR=7.75e-04
  Batch    0/48 | Loss: 2.6241 | LR: 7.74e-04 | GradNorm: 3.871
  Batch    4/48 | Loss: 2.6369 | LR: 7.71e-04 | GradNorm: 3.808
  Batch    8/48 | Loss: 2.6431 | LR: 7.67e-04 | GradNorm: 4.681
  Batch   12/48 | Loss: 2.6418 | LR: 7.64e-04 | GradNorm: 3.816
  Batch   16/48 | Loss: 2.5373 | LR: 7.61e-04 | GradNorm: 2.349
  Batch   20/48 | Loss: 2.5908 | LR: 7.57e-04 | GradNorm: 4.049
  Batch   24/48 | Loss: 2.7051 | LR: 7.54e-04 | GradNorm: 3.253
  Batch   28/48 | Loss: 2.4227 | LR: 7.50e-04 | GradNorm: 1.730
  Batch   32/48 | Loss: 2.5754 | LR: 7.47e-04 | GradNorm: 1.816
  Batch   36/48 | Loss: 2.5699 | LR: 7.43e-04 | GradNorm: 3.729
  Batch   40/48 | Loss: 2.5552 | LR: 7.40e-04 | GradNorm: 2.851
  Batch   44/48 | Loss: 2.5316 | LR: 7.36e-04 | GradNorm: 2.679
Epoch 11/30: Train=37.10%, Val=27.49%, Best=27.49% (E11), Gap=9.61%, LR=7.34e-04
  Batch    0/48 | Loss: 2.5159 | LR: 7.33e-04 | GradNorm: 2.005
  Batch    4/48 | Loss: 2.6191 | LR: 7.29e-04 | GradNorm: 4.331
  Batch    8/48 | Loss: 2.5442 | LR: 7.26e-04 | GradNorm: 2.519
  Batch   12/48 | Loss: 2.5751 | LR: 7.22e-04 | GradNorm: 4.755
  Batch   16/48 | Loss: 2.4887 | LR: 7.18e-04 | GradNorm: 2.562
  Batch   20/48 | Loss: 2.6212 | LR: 7.15e-04 | GradNorm: 4.206
  Batch   24/48 | Loss: 2.5649 | LR: 7.11e-04 | GradNorm: 4.529
  Batch   28/48 | Loss: 2.4430 | LR: 7.07e-04 | GradNorm: 1.611
  Batch   32/48 | Loss: 2.4678 | LR: 7.03e-04 | GradNorm: 3.487
  Batch   36/48 | Loss: 2.3939 | LR: 7.00e-04 | GradNorm: 3.650
  Batch   40/48 | Loss: 2.3790 | LR: 6.96e-04 | GradNorm: 2.688
  Batch   44/48 | Loss: 2.4240 | LR: 6.92e-04 | GradNorm: 4.568
‚ö†Ô∏è  Below trajectory threshold at epoch 12: 27.04% < 80.0% (Count: 1/3)
Epoch 12/30: Train=39.28%, Val=27.04%, Best=27.49% (E11), Gap=12.24%, LR=6.89e-04
  Batch    0/48 | Loss: 2.4137 | LR: 6.88e-04 | GradNorm: 2.182
  Batch    4/48 | Loss: 2.3831 | LR: 6.84e-04 | GradNorm: 2.815
  Batch    8/48 | Loss: 2.3112 | LR: 6.80e-04 | GradNorm: 3.272
  Batch   12/48 | Loss: 2.2971 | LR: 6.76e-04 | GradNorm: 3.474
  Batch   16/48 | Loss: 2.3196 | LR: 6.73e-04 | GradNorm: 2.470
  Batch   20/48 | Loss: 2.3775 | LR: 6.69e-04 | GradNorm: 3.112
  Batch   24/48 | Loss: 2.4823 | LR: 6.65e-04 | GradNorm: 5.082
  Batch   28/48 | Loss: 2.3465 | LR: 6.61e-04 | GradNorm: 1.770
  Batch   32/48 | Loss: 2.3372 | LR: 6.57e-04 | GradNorm: 3.954
  Batch   36/48 | Loss: 2.3218 | LR: 6.53e-04 | GradNorm: 2.741
  Batch   40/48 | Loss: 2.3745 | LR: 6.49e-04 | GradNorm: 4.160
  Batch   44/48 | Loss: 2.2964 | LR: 6.44e-04 | GradNorm: 3.522
‚ö†Ô∏è  Below trajectory threshold at epoch 13: 28.56% < 80.0% (Count: 2/3)
Epoch 13/30: Train=44.29%, Val=28.56%, Best=28.56% (E13), Gap=15.73%, LR=6.41e-04
  Batch    0/48 | Loss: 2.3764 | LR: 6.40e-04 | GradNorm: 3.435
  Batch    4/48 | Loss: 2.3730 | LR: 6.36e-04 | GradNorm: 5.674
  Batch    8/48 | Loss: 2.3887 | LR: 6.32e-04 | GradNorm: 4.232
  Batch   12/48 | Loss: 2.2827 | LR: 6.28e-04 | GradNorm: 2.849
  Batch   16/48 | Loss: 2.2773 | LR: 6.24e-04 | GradNorm: 2.733
  Batch   20/48 | Loss: 2.2127 | LR: 6.20e-04 | GradNorm: 3.051
  Batch   24/48 | Loss: 2.2737 | LR: 6.16e-04 | GradNorm: 3.481
  Batch   28/48 | Loss: 2.2990 | LR: 6.11e-04 | GradNorm: 3.881
  Batch   32/48 | Loss: 2.2613 | LR: 6.07e-04 | GradNorm: 3.397
  Batch   36/48 | Loss: 2.2470 | LR: 6.03e-04 | GradNorm: 2.416
  Batch   40/48 | Loss: 2.2444 | LR: 5.99e-04 | GradNorm: 3.427
  Batch   44/48 | Loss: 2.2304 | LR: 5.94e-04 | GradNorm: 3.447
‚ö†Ô∏è  Below trajectory threshold at epoch 14: 35.09% < 80.0% (Count: 3/3)
üõë Early stopping: Not following successful LAMB trajectory
   Expected: >80.0% by epoch 12, Got: 35.09%
Final best validation accuracy: 35.09% (epoch 14)
